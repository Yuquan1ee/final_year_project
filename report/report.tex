\documentclass[12pt,a4paper]{report}

% ============================================================
% Packages
% ============================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}                    % Times New Roman font
\usepackage[margin=1in]{geometry}     % 1 inch margins
\usepackage{graphicx}                 % Images
\usepackage{float}                    % Figure placement
\usepackage{caption}                  % Captions
\usepackage{subcaption}               % Subfigures
\usepackage{hyperref}                 % Hyperlinks
\usepackage{setspace}                 % Line spacing
\usepackage{titlesec}                 % Section formatting
\usepackage{tocloft}                  % TOC formatting
\usepackage{fancyhdr}                 % Headers and footers
\usepackage{tabularx}                 % Tables
\usepackage{booktabs}                 % Better table rules
\usepackage{enumitem}                 % List formatting
\usepackage{amsmath}                  % Math
\usepackage{listings}                 % Code listings
\usepackage{xcolor}                   % Colors
\usepackage{url}                      % URL formatting
\usepackage{microtype}                % Microtypographic enhancements (prevents margin overflow)
\usepackage{pdfpages}                 % Include PDF pages
\usepackage{longtable}                % Multi-page tables
\usepackage{array}                    % Extended column definitions
\usepackage[authoryear,round]{natbib}  % APA-style author-year citations

% ============================================================
% Configuration
% ============================================================
\graphicspath{{images/}}
\onehalfspacing
\emergencystretch=1em                 % Allow slightly looser spacing to prevent overfull boxes

% Hyperlink styling
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=black,
    urlcolor=blue,
    citecolor=black,
}

% Code listing style
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    tabsize=2,
    captionpos=b,
}

% Header/footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}

% Section numbering: use 1, 2, 3 instead of 0.1, 0.2
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\arabic{subsubsection}}

% Section formatting
\titleformat{\section}
  {\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}

% Remove chapter numbering (we use sections directly)
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{3}

% ============================================================
% Document
% ============================================================
\begin{document}

% ============================================================
% TITLE PAGE
% ============================================================
\begin{titlepage}
    \begin{center}
        \vspace*{1cm}

        % NTU Logo
        \includegraphics[width=0.35\textwidth]{ntu_logo.png}

        \vspace{1cm}

        {\Large \textbf{NANYANG TECHNOLOGICAL UNIVERSITY}}

        \vspace{0.5cm}

        {\large SCHOOL OF COMPUTER SCIENCE AND ENGINEERING}

        \vspace{2cm}

        {\LARGE \textbf{Diffusion Models for Intelligent\\Image Editing and Inpainting}}

        \vspace{2cm}

        {\large
        Lee Yu Quan\\
        }

        \vspace{2cm}

        {\large
        \textbf{Supervisor:} Prof Zhang Hanwang\\
        }

        \vspace{1cm}

        {\large
        School of Computer Science and Engineering\\
        }

        \vspace{1.5cm}

        {\large
        A Final Year Project report\\
        presented to Nanyang Technological University\\
        in partial fulfilment of the requirements for the\\
        degree of Bachelor of Engineering\\
        }

        \vspace{1cm}

        {\large 2025/2026}

    \end{center}
\end{titlepage}

% ============================================================
% ACKNOWLEDGEMENTS
% ============================================================
\pagenumbering{roman}
\setcounter{page}{2}

\section*{Acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

Over the course of two semesters working on this final year project, I would like to express my appreciation to everyone who has encouraged me and offered their guidance, helping make this project possible.

I would like to extend my sincere gratitude to my supervisor, Prof Zhang Hanwang, for granting me the freedom to steer the direction of this project. His trust and openness allowed me to explore a wide variety of diffusion models and techniques in the field of image generation, which greatly enriched both the project and my learning experience.

Lastly, I would like to thank my examiner, Prof (placeholder), for taking the time to review and evaluate this final year project.

\vspace{1cm}
\noindent Lee Yu Quan\\
March 2026

\newpage

% ============================================================
% TABLE OF CONTENTS
% ============================================================
\tableofcontents
\newpage

% ============================================================
% TABLE OF FIGURES
% ============================================================
\listoffigures
\addcontentsline{toc}{section}{Table of Figures}
\newpage

% ============================================================
% LIST OF TABLES (optional)
% ============================================================
\listoftables
\addcontentsline{toc}{section}{List of Tables}
\newpage

% ============================================================
% MAIN BODY
% ============================================================
\pagenumbering{arabic}
\setcounter{page}{1}

% ============================================================
% 1. INTRODUCTION
% ============================================================
\section{Introduction}

\subsection{Abstract}
% TODO: Write abstract

\subsection{Background and Motivation}

Large language models are the most discussed aspect of generative AI today, but image generation is not far behind. According to Grand View Research, the global AI image generator market was valued at USD 349.6 million in 2023 and is projected to grow at a compound annual growth rate (CAGR) of 17.7\% to reach USD 1.08 billion by 2030 \citep{grandviewresearch2023}. Yet, there remains a lack of comprehensive software that caters to this growing demand in an accessible manner.

Traditional methods in image inpainting, such as patch-based and exemplar-based approaches, have notable limitations in generating semantically meaningful content, particularly in high-resolution or complex scenarios \citep{ma2023uncertainty}. These methods often struggle with boundary artefacts when dealing with large masked regions due to insufficient constraints, resulting in visible seams and structurally inconsistent outputs \citep{ma2023uncertainty}. Such limitations create significant accessibility barriers, as current solutions frequently require extensive technical expertise and expensive software licences, effectively restricting advanced image editing capabilities to professional users.

The introduction of deep learning techniques, particularly diffusion models \citep{ho2020ddpm, song2021ddim, rombach2022ldm}, has led to significant improvements in image generation quality and semantic comprehension, enabling capabilities that were previously difficult or impossible to automate. A detailed review of these developments is presented in Section~\ref{sec:literature_review}.

Despite these breakthroughs, critical gaps persist between state-of-the-art image editing models and users' practical needs. Existing solutions face four key limitations: (1) fragmented ecosystems requiring users to switch between different applications for different editing tasks, (2) high complexity barriers that make advanced editing tools inaccessible to non-expert users, (3) reliance on command-line tools, Python programming, and GPU-equipped hardware, and (4) a lack of unified platforms that integrate multiple diffusion model capabilities within a single interface.

This project, DiffusionDesk, addresses these gaps by deploying a web-based platform that integrates diffusion models for inpainting, style transfer, and image restoration within a single, user-friendly interface. Developed as a Final Year Project at Nanyang Technological University under the supervision of Prof Zhang Hanwang, the application provides three core image editing capabilities:

\begin{enumerate}
    \item \textbf{Inpainting} -- removing or replacing objects within selected regions of an image using models such as Stable Diffusion, Stable Diffusion XL, Kandinsky, and FLUX.1 Fill.
    \item \textbf{Style Transfer} -- applying artistic styles such as anime, oil painting, and watercolour to images using diffusion-based image-to-image translation.
    \item \textbf{Restoration} -- enhancing image quality through face restoration (CodeFormer, GFPGAN) and image upscaling (Real-ESRGAN).
\end{enumerate}

By exposing these models through a FastAPI backend and a React-based browser frontend, DiffusionDesk aims to make diffusion-based image editing accessible to users without requiring direct interaction with the underlying models or command-line tools.

\subsection{Project Objective}

The objective of this project is to design and deploy a web-based image editing platform that leverages open-source diffusion models to provide intelligent inpainting, style transfer, and image restoration capabilities. The specific objectives are as follows:

\begin{enumerate}
    \item To develop a responsive web application that integrates multiple diffusion models for image editing within a unified interface.
    \item To implement an inpainting feature that enables users to selectively remove or replace objects in images using state-of-the-art diffusion models, including Stable Diffusion, Stable Diffusion XL, Kandinsky, and FLUX.1 Fill.
    \item To implement a style transfer feature that allows users to apply artistic styles to images through diffusion-based image-to-image translation.
    \item To implement an image restoration feature that enhances image quality through face restoration and upscaling using CodeFormer, GFPGAN, and Real-ESRGAN.
    \item To design an intuitive user interface that enables non-expert users to perform advanced image editing tasks without requiring technical expertise in machine learning or programming.
    \item To evaluate the system's performance through processing speed benchmarks, output quality assessments, and usability considerations.
\end{enumerate}

\subsection{Limitations}

This project is subject to the following limitations:

\begin{enumerate}
    \item \textbf{Open-source models only} -- The application exclusively utilises open-source diffusion models available through the Hugging Face ecosystem. Proprietary or commercially licensed models are not included, which may limit the range of available capabilities compared to commercial solutions.
    \item \textbf{GPU resource constraints} -- Diffusion model inference is computationally intensive and requires GPU acceleration. The available GPU memory (VRAM) constrains the size and complexity of models that can be loaded simultaneously. Quantisation techniques (4-bit, 8-bit) are employed to mitigate this, but may result in slight quality degradation.
    \item \textbf{Supported image formats} -- The application supports JPEG, JPG, and PNG image formats only. Other formats such as TIFF, BMP, WebP, or RAW are not supported.
    \item \textbf{No mobile application} -- The platform is designed as a web application accessible through desktop and mobile browsers. A dedicated native mobile application is not within the project scope.
    \item \textbf{No video processing} -- The system processes individual images only. Video frame processing, video inpainting, or video style transfer are not supported.
    \item \textbf{No 3D image manipulation} -- The application is limited to 2D image editing. 3D reconstruction, 3D-aware editing, or depth-based manipulation are not included.
    \item \textbf{Inference only} -- The project focuses on model inference using pre-trained models. Model training, fine-tuning, or custom model development are outside the project scope.
\end{enumerate}

\subsection{Project Scope}

The scope of this project encompasses the following:

\subsubsection{In Scope}
\begin{itemize}
    \item Development of a web-based frontend using React, TypeScript, and Tailwind CSS that provides an intuitive user interface for all three editing features.
    \item Development of a backend API using FastAPI and PyTorch that serves diffusion model inference for inpainting, style transfer, and image restoration.
    \item Implementation of inpainting using Stable Diffusion Inpainting, Stable Diffusion XL Inpainting, Kandinsky Inpainting, and FLUX.1 Fill models.
    \item Implementation of style transfer using SDXL image-to-image generation with artistic style prompts.
    \item Implementation of image restoration using CodeFormer, GFPGAN (face restoration), and Real-ESRGAN (image upscaling).
    \item Support for JPEG, JPG, and PNG image formats.
    \item VRAM optimisation through model quantisation (4-bit, 8-bit) and CPU offloading to accommodate varying GPU configurations.
    \item Deployment and testing on cloud GPU environments such as Google Colab.
\end{itemize}

\subsubsection{Out of Scope}
\begin{itemize}
    \item Native mobile application development.
    \item Video processing, video inpainting, or video style transfer.
    \item 3D image manipulation or depth-based editing.
    \item Model training, fine-tuning, or custom model development.
    \item User authentication, user account management, or multi-user collaboration features.
    \item Image formats other than JPEG, JPG, and PNG.
\end{itemize}

Success will be measured through processing speed benchmarks, output quality assessments, and user experience evaluation across the three core features.

\newpage

% ============================================================
% 2. PROJECT SCHEDULE
% ============================================================
\section{Project Schedule}

This section outlines the project timeline, work breakdown structure, and risk management plan for DiffusionDesk. The project spans two semesters of Academic Year 2025/2026, with key milestones aligned to the FYP submission deadlines.

\subsection{Project Timeline}

Table~\ref{tab:timeline} presents the key milestones and deliverables for the project.

\begin{table}[H]
\centering
\caption{Project Timeline and Milestones}
\label{tab:timeline}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{3cm}|p{3cm}|p{7.5cm}|}
\hline
\textbf{Date} & \textbf{Week} & \textbf{Milestone} \\
\hline
11 Aug 2025 & Sem 1, Wk 1 & FYP officially commences \\
\hline
1 Sep 2025 & Sem 1, Wk 4 & Submission of Project Plan to Supervisor \\
\hline
Oct 2025 & Sem 1, Wk 8--10 & Backend API and diffusion service implementation \\
\hline
Nov 2025 & Sem 1, Wk 12--13 & Frontend development and API integration \\
\hline
26 Jan 2026 & Sem 2, Wk 3 & Submission of Interim Report \\
\hline
Feb 2026 & Sem 2, Wk 5--7 & Feature refinement and testing on cloud GPU \\
\hline
23 Mar 2026 & Sem 2, Wk 10 & Submission of Final Report \\
\hline
17 Apr 2026 & Sem 2, Wk 13 & Submission of Amended Final Report \\
\hline
8--13 May 2026 & -- & Oral Presentation (20 min + 10 min Q\&A) \\
\hline
\end{tabular}
\end{table}

\subsection{Work Breakdown}

This section provides a structured breakdown of the project activities, their descriptions, estimated effort, and dependencies. The work breakdown structure facilitates progress tracking and resource allocation throughout the development lifecycle.

\small
\renewcommand{\arraystretch}{1.3}
\begin{longtable}{|p{0.6cm}|p{2.6cm}|p{5.8cm}|p{1.4cm}|p{1.8cm}|}
\caption{Work Breakdown Structure} \label{tab:wbs} \\
\hline
\textbf{ID} & \textbf{Activity} & \textbf{Description} & \textbf{Effort (Days)} & \textbf{Depends On} \\
\hline
\endfirsthead
\hline
\textbf{ID} & \textbf{Activity} & \textbf{Description} & \textbf{Effort (Days)} & \textbf{Depends On} \\
\hline
\endhead
\hline
\endfoot

1.1 & Literature Review & Review foundational papers on diffusion models (DDPM, DDIM, LDM) and existing inpainting, style transfer, and restoration techniques. & 10 & -- \\
\hline
1.2 & Technology Evaluation & Evaluate and select appropriate frameworks, libraries, and pre-trained models from the Hugging Face ecosystem. & 5 & 1.1 \\
\hline
1.3 & Requirements Specification & Define functional and non-functional requirements based on project objectives and supervisor feedback. & 4 & 1.2 \\
\hline
2.1 & Backend Architecture Design & Design the FastAPI backend structure, including API endpoints, service layers, and model management strategy. & 6 & 1.3 \\
\hline
2.2 & Inpainting Service & Implement the inpainting service supporting SD, SDXL, Kandinsky, and FLUX.1 Fill models with quantisation support. & 15 & 2.1 \\
\hline
2.3 & Style Transfer Service & Implement the style transfer service using SDXL img2img with configurable style prompts and parameters. & 10 & 2.1 \\
\hline
2.4 & Restoration Service & Implement face restoration (CodeFormer, GFPGAN) and image upscaling (Real-ESRGAN) services. & 10 & 2.1 \\
\hline
2.5 & VRAM Optimisation & Implement model quantisation (4-bit, 8-bit) and CPU offloading strategies to support varying GPU configurations. & 8 & 2.2, 2.3 \\
\hline
3.1 & Frontend UI Design & Design the user interface layout, including wireframes for the Home, Inpainting, Style Transfer, and Restoration tabs. & 5 & 1.3 \\
\hline
3.2 & Frontend Implementation & Develop the React frontend with TypeScript and Tailwind CSS, implementing all UI components and tab navigation. & 15 & 3.1 \\
\hline
3.3 & Canvas and Mask Drawing & Implement the interactive canvas component for users to draw inpainting masks on uploaded images. & 8 & 3.2 \\
\hline
3.4 & API Integration & Connect the frontend to the backend API, implementing image upload, processing requests, and result display. & 6 & 2.2--2.4, 3.2 \\
\hline
4.1 & Cloud Deployment Testing & Deploy and test the backend on Google Colab with ngrok tunnelling to validate GPU inference performance. & 5 & 3.4 \\
\hline
4.2 & Feature Testing & Conduct end-to-end testing of all features, addressing bugs and refining based on test results. & 10 & 4.1 \\
\hline
4.3 & Performance Benchmarking & Measure and document inference times, memory usage, and output quality across different models. & 5 & 4.2 \\
\hline
5.1 & Supervisor Meetings & Regular meetings with the supervisor to report progress, seek guidance, and align on project direction. & 8 & All \\
\hline
5.2 & Interim Report Writing & Prepare and submit the interim report documenting progress, challenges, and preliminary results. & 5 & 2.5, 3.4 \\
\hline
5.3 & Final Report Writing & Prepare the final report with comprehensive documentation of system design, implementation, and evaluation. & 15 & 4.3, 5.2 \\
\hline
5.4 & Oral Presentation Prep & Prepare presentation slides and rehearse for the oral examination. & 5 & 5.3 \\
\hline

\end{longtable}
\normalsize

\subsection{Risk Management}

This section identifies potential risks that may impact the project's success and outlines mitigation strategies. Each risk is assessed based on its probability of occurrence, potential impact, and overall risk level. Proactive risk management ensures that challenges are anticipated and addressed promptly.

\footnotesize
\renewcommand{\arraystretch}{1.4}
\setlength{\tabcolsep}{5pt}
\begin{longtable}{|>{\raggedright\arraybackslash}p{2.2cm}|>{\raggedright\arraybackslash}p{6.2cm}|c|c|c|}
\caption{Risk Management Plan} \label{tab:risk} \\
\hline
\textbf{Risk} & \textbf{Mitigation Strategy} & \textbf{Prob.} & \textbf{Impact} & \textbf{Level} \\
\hline
\endfirsthead
\hline
\textbf{Risk} & \textbf{Mitigation Strategy} & \textbf{Prob.} & \textbf{Impact} & \textbf{Level} \\
\hline
\endhead
\hline
\endfoot

Insufficient GPU memory for large models & Implement model quantisation (4-bit, 8-bit) using bitsandbytes and enable CPU offloading. Prioritise smaller models when VRAM is limited. & High & High & High \\
\hline
Slow model inference & Use DDIM sampling with reduced steps (20--50), enable attention slicing, and implement model caching to reduce loading overhead. & Medium & High & Medium \\
\hline
Low-quality or artefacted outputs & Fine-tune prompt engineering, adjust guidance scale and denoising strength, and provide users with parameter controls for iterative refinement. & Medium & Medium & Medium \\
\hline
Diffusers library breaking changes & Pin specific library versions in requirements.txt and test compatibility before updating dependencies. & Medium & Medium & Medium \\
\hline
Cloud GPU constraints (Colab limits) & Design for stateless operation, allowing sessions to restart without data loss. Document alternative deployment options (NTU HPC, local GPU). & Medium & Medium & Medium \\
\hline
Frontend-backend integration issues & Define clear API contracts using OpenAPI, implement comprehensive error handling, and test integration incrementally. & Medium & High & Medium \\
\hline
Scope creep & Adhere strictly to defined scope. Evaluate new requests against timeline constraints and prioritise core features. & Medium & Medium & Medium \\
\hline
Unfamiliarity with diffusion architectures & Conduct thorough literature review early. Leverage tutorials, documentation, and pre-trained models to accelerate learning. & Low & High & Low \\
\hline
Time management challenges & Maintain detailed project schedule with milestones. Allocate buffer time and communicate proactively with supervisor. & Low & High & Low \\
\hline

\end{longtable}
\normalsize
\setlength{\tabcolsep}{6pt}

\newpage

% ============================================================
% 3. LITERATURE REVIEW
% ============================================================
\section{Literature Review}
\label{sec:literature_review}

\subsection{Diffusion Models}

The foundation of modern image generation lies in diffusion models, which produce images through an iterative denoising process. This subsection reviews the key developments that underpin the models used in this project.

\subsubsection{Denoising Diffusion Probabilistic Models (DDPM)}

\citet{ho2020ddpm} proposed Denoising Diffusion Probabilistic Models (DDPMs), which generate images by treating the process as a series of denoising steps grounded in nonequilibrium thermodynamics. The forward process gradually adds Gaussian noise to an image over $T$ timesteps until the image becomes pure noise. The reverse process then learns to denoise step by step, recovering a clean image from random noise. DDPMs demonstrated image quality that surpassed the then-dominant Generative Adversarial Networks (GANs), producing diverse, high-fidelity samples without the training instability commonly associated with GANs. However, the original DDPM formulation required a large number of denoising steps (typically $T = 1000$), resulting in slow sampling speeds.

\subsubsection{Denoising Diffusion Implicit Models (DDIM)}

\citet{song2021ddim} addressed the slow sampling limitation of DDPMs by proposing Denoising Diffusion Implicit Models (DDIMs). DDIMs reformulate the reverse diffusion process as a non-Markovian process, meaning that each denoising step can depend on the original noisy input rather than solely on the immediately preceding step. This reformulation allows for deterministic sampling and, crucially, enables the use of a subsequence of only 20--100 steps while maintaining comparable image quality. The result is a 10--50$\times$ speedup over DDPMs, making diffusion-based generation significantly more practical for interactive applications.

\subsubsection{Latent Diffusion Models (LDM)}

\citet{rombach2022ldm} proposed Latent Diffusion Models (LDMs), which achieved an optimal balance between generative quality and computational efficiency. Rather than performing diffusion directly in pixel space, LDMs first encode images into a lower-dimensional latent representation using a pre-trained autoencoder, then apply the diffusion process within this compressed latent space. This approach alleviates critical computational bottlenecks, substantially reducing memory and computation requirements while preserving high image quality. LDMs form the basis of the widely adopted Stable Diffusion family of models, including the inpainting and image-to-image variants used in this project.

\subsubsection{Enabling Technologies for Conditional Generation}

The diffusion models reviewed above provide the foundational denoising mechanism, but practical text-to-image systems require additional components for conditional generation. This subsection reviews three key enabling technologies that bridge the gap between unconditional diffusion and controllable image synthesis.

\paragraph{Classifier-Free Guidance}

\citet{ho2022cfg} introduced classifier-free guidance, a technique that enables conditional diffusion models to produce outputs strongly aligned with input conditions (e.g., text prompts) without requiring a separate classifier network. The method trains a single neural network to perform both conditional and unconditional generation by randomly dropping the conditioning signal during training. At inference time, the model's output is extrapolated away from the unconditional prediction towards the conditional prediction, controlled by a guidance scale parameter $w$:
\begin{equation}
\tilde{\epsilon}_\theta(z_t, c) = \epsilon_\theta(z_t, \emptyset) + w \cdot (\epsilon_\theta(z_t, c) - \epsilon_\theta(z_t, \emptyset))
\end{equation}
where $\epsilon_\theta(z_t, c)$ is the conditional prediction, $\epsilon_\theta(z_t, \emptyset)$ is the unconditional prediction, and $w$ is the guidance scale. Higher values of $w$ produce outputs more faithful to the conditioning but may reduce diversity and introduce artefacts. This parameter is exposed in DiffusionDesk as ``guidance scale'' and is fundamental to all inpainting and style transfer operations.

\paragraph{CLIP Text-Image Alignment}

\citet{radford2021clip} developed CLIP (Contrastive Language-Image Pre-training), a model trained on 400 million image-text pairs to learn a joint embedding space for images and text. CLIP's text encoder transforms natural language prompts into dense vector representations that can guide image generation. In Stable Diffusion and similar models, the CLIP text encoder processes user prompts into conditioning embeddings that direct the denoising process. This architecture enables intuitive text-based control: users describe their desired output in natural language, and the model generates images aligned with that description. SDXL extends this by using dual text encoders (CLIP ViT-L and OpenCLIP ViT-G) for richer text understanding.

\paragraph{Diffusion Transformers (DiT)}

While Stable Diffusion and SDXL use U-Net architectures as their denoising backbone, \citet{peebles2023dit} demonstrated that Vision Transformers can serve as effective diffusion backbones. Diffusion Transformers (DiT) replace the convolutional U-Net with a transformer architecture, operating on sequences of latent patches. This approach scales more efficiently with model size and has led to state-of-the-art image generation quality. FLUX.1, developed by Black Forest Labs, adopts a DiT-based architecture with flow matching (a generalisation of diffusion that learns direct probability paths rather than score functions), achieving superior quality at the cost of increased computational requirements. The architectural shift from U-Net to DiT represents a significant evolution in diffusion model design.

\vspace{1em}
\noindent\textit{Having established the theoretical foundations---from DDPM's denoising mechanism, through DDIM's accelerated sampling, to LDM's latent space operation and the enabling technologies of classifier-free guidance, CLIP conditioning, and transformer backbones---the following sections examine how these principles are instantiated in practical models for specific image editing tasks.}

\subsection{Diffusion-Based Image Inpainting}

Image inpainting is the task of filling in missing or masked regions of an image with plausible content. Traditional approaches include patch-based methods that copy similar patches from elsewhere in the image, and exemplar-based techniques that iteratively fill regions based on surrounding texture and structure. While effective for simple cases, these methods struggle with large masked regions, complex scenes, and semantically meaningful content generation---they cannot, for example, intelligently fill a masked region with a contextually appropriate object.

Deep learning approaches, particularly diffusion models, have transformed inpainting by learning rich semantic priors from large datasets. Rather than relying on local texture matching, diffusion-based inpainting models understand scene context and can generate novel content that is both visually coherent and semantically meaningful. This subsection reviews four diffusion-based inpainting models implemented in DiffusionDesk, each building upon the theoretical foundations established in Section~3.1.

\subsubsection{Problem Definition}

Given an input image $x$ and a binary mask $m$ indicating the region to be inpainted, the goal is to generate an output image $\hat{x}$ where the masked region contains plausible content consistent with the surrounding context. In diffusion-based inpainting, the model is additionally conditioned on a text prompt $c$ that guides the generation. The inpainting process can be formulated as:
\begin{equation}
\hat{x} = f(x, m, c; \theta)
\end{equation}
where $f$ is the inpainting model with parameters $\theta$. The model must preserve the unmasked regions exactly while generating coherent content in the masked areas.

\subsubsection{Inpainting Model Architectures}

\paragraph{Stable Diffusion Inpainting}

Stable Diffusion Inpainting \citep{rombach2022ldm} extends the base Stable Diffusion v1.5 model for inpainting by modifying the input architecture. While the original model accepts a 4-channel latent input, the inpainting variant accepts 9 channels: 4 for the noisy latent, 4 for the encoded masked image, and 1 for the downsampled mask. This architectural modification allows the model to explicitly condition on both the masked image content and the mask geometry.

The model inherits the core DDPM/DDIM denoising mechanism and operates in the latent space as per LDM principles. Text conditioning is provided through the CLIP ViT-L/14 text encoder, and classifier-free guidance controls the fidelity to the text prompt. With approximately 860 million parameters and VRAM requirements of 5--7 GB in FP16 precision, Stable Diffusion Inpainting offers a good balance of quality and computational efficiency, making it suitable for users with limited GPU resources.

\paragraph{Stable Diffusion XL Inpainting}

SDXL Inpainting builds upon the base SDXL architecture, which introduced several improvements over Stable Diffusion v1.5: a larger U-Net backbone with approximately 2.6 billion parameters, native 1024$\times$1024 resolution support, and dual text encoders (CLIP ViT-L and OpenCLIP ViT-G) for enhanced prompt understanding. These improvements translate to higher-quality inpainting results with better detail preservation and more accurate text-to-image alignment.

The theoretical foundations remain consistent: SDXL Inpainting uses DDPM-based denoising, supports DDIM and other accelerated schedulers, operates in latent space, and employs classifier-free guidance. However, the increased model capacity comes at a computational cost---SDXL Inpainting requires 10--12 GB VRAM in FP16 precision. To address this, DiffusionDesk implements 8-bit and 4-bit quantisation using the bitsandbytes library, reducing VRAM requirements to approximately 6 GB and 4 GB respectively, with minimal quality degradation.

\paragraph{Kandinsky 2.2 Inpainting}

Kandinsky 2.2 employs a distinct two-stage architecture inspired by unCLIP/DALL-E 2. The first stage (``prior'') maps the text prompt to a CLIP image embedding, predicting what the image embedding should look like given the text. The second stage (``decoder'') generates the actual image conditioned on both the text and the predicted image embedding. This architecture leverages CLIP's powerful joint text-image space to guide generation.

For inpainting, Kandinsky conditions the decoder on the masked image and mask geometry in addition to the CLIP embeddings. With approximately 2 billion parameters across both stages and VRAM requirements of 6--8 GB, Kandinsky offers comparable resource usage to Stable Diffusion while providing different aesthetic characteristics. The two-stage approach can produce results with strong text alignment due to the explicit CLIP image embedding conditioning.

\paragraph{FLUX.1 Fill}

FLUX.1 Fill, developed by Black Forest Labs, represents a significant architectural departure from the U-Net-based models. Built on the Diffusion Transformer (DiT) architecture, FLUX.1 uses a transformer backbone operating on sequences of latent patches rather than a convolutional U-Net. Additionally, FLUX.1 employs flow matching rather than traditional score-based diffusion, learning direct probability paths between noise and data distributions.

With approximately 12 billion parameters and a T5-XXL text encoder (rather than CLIP), FLUX.1 Fill achieves state-of-the-art inpainting quality with exceptional detail, coherence, and prompt following. However, this quality comes at significant computational cost: full BF16 inference requires 22--24 GB VRAM. DiffusionDesk implements NF4 quantisation to reduce this to approximately 10 GB, enabling deployment on consumer GPUs like the NVIDIA T4 available in Google Colab.

\subsubsection{Model Comparison}

Table~\ref{tab:inpainting_comparison} summarises the key characteristics of the four inpainting models implemented in DiffusionDesk.

\begin{table}[H]
\centering
\caption{Comparison of Inpainting Models}
\label{tab:inpainting_comparison}
\small
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{2.2cm}|p{2.5cm}|p{2.8cm}|p{2.5cm}|p{2.8cm}|}
\hline
\textbf{Criteria} & \textbf{SD Inpainting} & \textbf{SDXL Inpainting} & \textbf{Kandinsky 2.2} & \textbf{FLUX.1 Fill} \\
\hline
Architecture & U-Net (LDM) & U-Net (LDM) & Prior + Decoder & DiT (Transformer) \\
\hline
Parameters & $\sim$860M & $\sim$2.6B & $\sim$2B & $\sim$12B \\
\hline
Text Encoder & CLIP ViT-L & CLIP + OpenCLIP & CLIP & T5-XXL \\
\hline
VRAM (FP16) & 5--7 GB & 10--12 GB & 6--8 GB & 22--24 GB \\
\hline
VRAM (4-bit) & N/A & $\sim$4 GB & N/A & $\sim$10 GB \\
\hline
Quality & Good & Very Good & Good & Excellent \\
\hline
Theory Basis & DDPM, DDIM, LDM, CFG & DDPM, DDIM, LDM, CFG & DDPM, DDIM, LDM, CLIP & Flow Matching, DiT \\
\hline
\end{tabular}
\end{table}

\subsubsection{Selection Justification}

DiffusionDesk includes all four inpainting models to provide users with flexibility across different use cases and hardware constraints:

\begin{itemize}
    \item \textbf{SD Inpainting}: Entry-level option for users with limited VRAM ($<$8 GB). Provides good quality results with fast inference.
    \item \textbf{SDXL Inpainting}: Recommended default for users with mid-range GPUs (8--12 GB). Offers improved quality and prompt understanding with quantisation options for constrained environments.
    \item \textbf{Kandinsky 2.2}: Alternative architecture providing different aesthetic characteristics. Useful when SDXL results are unsatisfactory for specific prompts.
    \item \textbf{FLUX.1 Fill}: State-of-the-art quality for users with high-end GPUs or when quantised deployment is acceptable. Best choice for complex inpainting tasks requiring maximum coherence.
\end{itemize}

\noindent\textit{The diffusion architectures examined for inpainting can also be applied to style transfer. Rather than filling masked regions, style transfer leverages the same denoising process to transform an entire image's aesthetic while preserving its structure.}

\subsection{Diffusion-Based Style Transfer}

Style transfer aims to render an image in a different artistic style while preserving its underlying content and structure. This section reviews the evolution of neural style transfer techniques and explains how diffusion models provide a powerful and flexible approach through the image-to-image (img2img) mechanism.

\subsubsection{Evolution of Neural Style Transfer}

\citet{gatys2016style} pioneered neural style transfer by demonstrating that convolutional neural networks (CNNs) encode both content and style information in their hierarchical feature representations. Their optimisation-based approach iteratively modifies an image to match the content features of a source image and the style features (captured via Gram matrices) of a reference style image. While producing impressive results, this method requires a slow optimisation process for each image pair.

Subsequent work developed feed-forward style transfer networks that train a single network per style, enabling real-time inference. However, these methods are limited to pre-defined styles and cannot generalise to arbitrary style descriptions. More recent approaches explored arbitrary style transfer using adaptive instance normalisation or attention mechanisms, but these often struggle with complex style semantics or structural preservation.

Diffusion models offer a compelling alternative: rather than explicitly separating content and style features, they leverage the denoising process with text conditioning to apply stylistic transformations described in natural language. This approach supports arbitrary styles without retraining and benefits from the semantic understanding encoded in large-scale text-image models.

\subsubsection{Image-to-Image Mechanism}

Diffusion-based style transfer operates through the image-to-image (img2img) pipeline, which differs fundamentally from text-to-image generation. Rather than starting from pure random noise, img2img begins by adding controlled noise to the input image and then denoises with a style-describing prompt. This process can be understood as follows:

\begin{enumerate}
    \item \textbf{Encoding}: The input image is encoded into latent space using the VAE encoder.
    \item \textbf{Noise Addition}: Gaussian noise is added to the latent representation according to a specified ``denoising strength'' parameter $s \in [0, 1]$. Higher values add more noise, starting the denoising from a later timestep.
    \item \textbf{Conditional Denoising}: The noised latent is denoised using the diffusion model, conditioned on a text prompt describing the desired style (e.g., ``oil painting style'', ``anime illustration'').
    \item \textbf{Decoding}: The denoised latent is decoded back to pixel space.
\end{enumerate}

The denoising strength parameter directly controls the trade-off between structure preservation and style application:
\begin{itemize}
    \item \textbf{Low strength (0.2--0.4)}: Subtle stylisation; original composition, shapes, and details are largely preserved.
    \item \textbf{Medium strength (0.5--0.7)}: Moderate stylisation; structural elements remain recognisable but undergo significant aesthetic transformation.
    \item \textbf{High strength (0.8--1.0)}: Aggressive stylisation; the output may diverge substantially from the input, using it primarily as compositional guidance.
\end{itemize}

This mechanism directly leverages the DDPM/DDIM denoising process: the noise schedule determines at which timestep the denoising begins, and classifier-free guidance ensures the output adheres to the style prompt.

\subsubsection{Approach Selection}

DiffusionDesk implements style transfer using SDXL img2img for the following reasons:

\begin{itemize}
    \item \textbf{Quality}: SDXL's larger capacity and dual text encoders produce higher-quality stylisations with better detail and prompt following compared to SD 1.5.
    \item \textbf{Resolution}: Native 1024$\times$1024 support enables high-resolution style transfer without tiling artefacts.
    \item \textbf{Flexibility}: Text-based style conditioning supports arbitrary styles (anime, oil painting, watercolour, pencil sketch, cyberpunk, etc.) without requiring style reference images.
    \item \textbf{Parameter Control}: Exposing denoising strength, guidance scale, and inference steps gives users fine-grained control over the structure-style trade-off.
    \item \textbf{Quantisation Support}: 8-bit and 4-bit quantisation via bitsandbytes enables deployment on resource-constrained GPUs.
\end{itemize}

DiffusionDesk provides preset style prompts (anime, oil painting, watercolour, pencil sketch, digital art) while allowing users to specify custom prompts for specialised styles.

\vspace{1em}
\noindent\textit{While diffusion models excel at generative tasks like inpainting and style transfer, image restoration requires specialised architectures optimised for specific degradation types. The following section examines restoration models that complement diffusion-based editing.}

\subsection{Image Restoration}

Image restoration addresses the inverse problem of recovering high-quality images from degraded observations. Common degradations include compression artefacts, noise, blur, low resolution, and facial damage (wrinkles, scratches, low quality). Unlike the generative tasks of inpainting and style transfer, restoration aims to recover or enhance existing content rather than synthesise new content.

Notably, the restoration models reviewed in this section are \textit{not} diffusion models. They employ distinct architectures---codebook-based transformers, generative adversarial networks (GANs), and enhanced super-resolution networks---each optimised for specific restoration tasks. DiffusionDesk includes these models to provide a comprehensive image editing pipeline that addresses both generative and restorative user needs.

\subsubsection{Face Restoration}

Face restoration is a specialised task that recovers high-quality facial details from degraded face images. The challenge lies in reconstructing plausible facial features (eyes, nose, mouth) while maintaining the subject's identity. DiffusionDesk implements two complementary approaches: CodeFormer and GFPGAN.

\paragraph{CodeFormer}

\citet{zhou2022codeformer} proposed CodeFormer, a transformer-based face restoration method that leverages a learned discrete codebook of high-quality facial features. The approach consists of three components:

\begin{enumerate}
    \item \textbf{Codebook Learning}: A vector-quantised autoencoder learns a discrete codebook capturing diverse high-quality facial features from a large face dataset.
    \item \textbf{Code Prediction}: Given a degraded face image, a transformer predicts the sequence of codebook indices that best represent the underlying face.
    \item \textbf{Controllable Decoding}: The decoder reconstructs the face from the predicted codes, with a fidelity parameter $w \in [0, 1]$ controlling the trade-off between restoration quality and identity preservation.
\end{enumerate}

The fidelity parameter is particularly valuable: setting $w = 0$ produces maximum restoration quality (potentially altering facial features), while $w = 1$ maximally preserves the input identity (with less aggressive restoration). Users can adjust this parameter based on whether they prioritise visual quality or identity fidelity.

CodeFormer excels at restoring severely degraded faces, producing natural-looking results with realistic textures. It handles diverse degradation types robustly and provides controllable output through the fidelity parameter.

\paragraph{GFPGAN}

\citet{wang2021gfpgan} developed GFPGAN (Generative Facial Prior GAN), which incorporates rich facial priors from a pre-trained face generation model (StyleGAN2) to assist face restoration. The key innovations include:

\begin{enumerate}
    \item \textbf{Generative Facial Prior}: Channel-split spatial feature transforms inject pre-trained StyleGAN2 features to provide high-quality facial priors.
    \item \textbf{Facial Component Dictionaries}: Pre-computed dictionaries of facial components (left eye, right eye, mouth) enable component-specific enhancement.
    \item \textbf{Identity-Preserving Loss}: An identity loss term encourages the restored face to match the input identity.
\end{enumerate}

GFPGAN tends to produce sharper results than CodeFormer in some cases but may occasionally alter facial features more aggressively. It performs particularly well on moderately degraded faces and old photographs.

\paragraph{Face Restoration Comparison}

Table~\ref{tab:face_restoration} compares CodeFormer and GFPGAN across key characteristics.

\begin{table}[H]
\centering
\caption{Comparison of Face Restoration Models}
\label{tab:face_restoration}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{3.5cm}|p{5cm}|p{5cm}|}
\hline
\textbf{Criteria} & \textbf{CodeFormer} & \textbf{GFPGAN} \\
\hline
Architecture & Transformer + Codebook & GAN + StyleGAN2 Prior \\
\hline
Controllability & Fidelity parameter (0--1) & Fixed \\
\hline
Severe Degradation & Excellent & Good \\
\hline
Identity Preservation & Controllable & Moderate \\
\hline
Output Sharpness & Natural & Sharp \\
\hline
VRAM Usage & 2--4 GB & 2--4 GB \\
\hline
\end{tabular}
\end{table}

DiffusionDesk includes both models to accommodate different user preferences and degradation scenarios. CodeFormer is recommended for severely degraded images or when identity preservation is critical, while GFPGAN is suitable for general enhancement of moderately degraded faces.

\subsubsection{Image Upscaling}

Image upscaling (super-resolution) increases image resolution while adding plausible high-frequency details. Classical interpolation methods (bicubic, bilinear) produce blurry results, while deep learning approaches can hallucinate realistic details.

\paragraph{Real-ESRGAN}

\citet{wang2021realesrgan} extended ESRGAN (Enhanced Super-Resolution GAN) to handle real-world degradations. Unlike prior methods trained on synthetic bicubic downsampling, Real-ESRGAN models a comprehensive degradation pipeline including blur, noise, compression, and their combinations. This training strategy enables robust performance on diverse real-world images.

Key characteristics of Real-ESRGAN include:

\begin{itemize}
    \item \textbf{RRDB Architecture}: Residual-in-Residual Dense Blocks provide powerful feature extraction for upscaling.
    \item \textbf{Real-World Degradation Model}: Training includes blur kernels, noise injection, JPEG compression, and resize operations in various orders.
    \item \textbf{Scale Options}: Supports 2$\times$ and 4$\times$ upscaling with pre-trained models.
    \item \textbf{Face Enhancement Integration}: A face-enhanced variant (Real-ESRGAN + GFPGAN) applies face restoration after upscaling for improved facial detail.
\end{itemize}

Real-ESRGAN produces sharp, detailed upscaled images with minimal artefacts, making it suitable for enhancing low-resolution photographs, enlarging images for printing, or improving image quality before further editing.

\subsubsection{Selection Justification}

DiffusionDesk includes CodeFormer, GFPGAN, and Real-ESRGAN to provide comprehensive restoration capabilities:

\begin{itemize}
    \item \textbf{CodeFormer}: Primary face restoration model offering controllable quality-fidelity trade-off.
    \item \textbf{GFPGAN}: Alternative face restoration for users preferring sharper outputs or as a fallback when CodeFormer results are unsatisfactory.
    \item \textbf{Real-ESRGAN}: Essential for image upscaling, complementing face restoration for full-image enhancement pipelines.
\end{itemize}

These restoration models integrate seamlessly with diffusion-based editing: users can upscale low-resolution images before inpainting, or restore faces after style transfer to recover facial details.

\subsection{Technology Stack}

This section reviews and justifies the technology choices for developing DiffusionDesk. Each subsection follows a structured approach: exploring available options, comparing them against relevant criteria, and justifying the final selection.

\subsubsection{Machine Learning Framework Selection}

\paragraph{Options Explored}

Two major deep learning frameworks were considered for model inference:
\begin{itemize}
    \item \textbf{PyTorch}: Developed by Meta AI, known for dynamic computation graphs, Pythonic API, and strong research community adoption.
    \item \textbf{TensorFlow}: Developed by Google, known for production deployment tools, static graph optimisation, and TensorFlow Serving.
\end{itemize}

\paragraph{Comparison}

\begin{table}[H]
\centering
\caption{ML Framework Comparison}
\label{tab:ml_framework}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{4cm}|p{4.5cm}|p{4.5cm}|}
\hline
\textbf{Criteria} & \textbf{PyTorch} & \textbf{TensorFlow} \\
\hline
Diffusion Model Support & Excellent (Diffusers, native) & Limited (some ports exist) \\
\hline
Pre-trained Models & Extensive (Hugging Face Hub) & Moderate \\
\hline
Dynamic Graphs & Native & TF2 eager mode (added later) \\
\hline
Debugging & Straightforward (Python) & More complex (graph mode) \\
\hline
Research Adoption & Dominant in generative AI & Strong in production ML \\
\hline
Quantisation Libraries & bitsandbytes, GPTQ, AWQ & TensorFlow Lite \\
\hline
\end{tabular}
\end{table}

\paragraph{Decision}

PyTorch was selected as the ML framework for the following reasons:
\begin{itemize}
    \item \textbf{Diffusers Library}: The Hugging Face Diffusers library, which provides unified pipeline abstractions for all diffusion models used in this project, is built on PyTorch.
    \item \textbf{Model Availability}: All target models (SD, SDXL, Kandinsky, FLUX.1, CodeFormer, GFPGAN, Real-ESRGAN) have official PyTorch implementations available on Hugging Face Hub.
    \item \textbf{Quantisation Support}: The bitsandbytes library for 8-bit and 4-bit quantisation is PyTorch-native and essential for deploying large models on resource-constrained GPUs.
    \item \textbf{Research Alignment}: PyTorch dominates generative AI research, ensuring access to the latest models and techniques.
\end{itemize}

\paragraph{Supporting Libraries}

The following PyTorch ecosystem libraries are utilised:
\begin{itemize}
    \item \textbf{Diffusers}: Unified pipeline API for diffusion models, scheduler implementations (DDIM, DPM++, Euler), and model loading utilities.
    \item \textbf{Transformers}: Text encoder loading (CLIP, T5) for conditioning diffusion models.
    \item \textbf{bitsandbytes}: 8-bit (LLM.int8) and 4-bit (NF4) quantisation for reduced VRAM usage.
    \item \textbf{Accelerate}: Device placement and mixed-precision inference utilities.
\end{itemize}

\subsubsection{Backend Framework Selection}

\paragraph{Options Explored}

Three Python web frameworks were considered for the API backend:
\begin{itemize}
    \item \textbf{FastAPI}: Modern, async-first framework with automatic OpenAPI documentation.
    \item \textbf{Flask}: Lightweight, mature framework with extensive ecosystem.
    \item \textbf{Django}: Full-featured framework with ORM, admin interface, and batteries-included philosophy.
\end{itemize}

\paragraph{Comparison}

\begin{table}[H]
\centering
\caption{Backend Framework Comparison}
\label{tab:backend_framework}
\small
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{2.8cm}|p{3.5cm}|p{3.2cm}|p{3.5cm}|}
\hline
\textbf{Criteria} & \textbf{FastAPI} & \textbf{Flask} & \textbf{Django} \\
\hline
Async Support & Native (ASGI) & Extension (async views) & Limited (ASGI adapter) \\
\hline
Type Hints & Native (Pydantic) & Manual & Manual \\
\hline
Auto Documentation & OpenAPI + Swagger UI & Extension required & Extension required \\
\hline
Performance & High (async I/O) & Moderate & Moderate \\
\hline
Learning Curve & Low & Low & Moderate \\
\hline
ML Ecosystem Fit & Excellent & Good & Moderate (ORM overhead) \\
\hline
\end{tabular}
\end{table}

\paragraph{Decision}

FastAPI was selected for the following reasons:
\begin{itemize}
    \item \textbf{Async Support}: Native async/await enables efficient handling of long-running inference requests without blocking other clients.
    \item \textbf{Pydantic Integration}: Type-validated request/response models ensure robust API contracts and clear documentation.
    \item \textbf{Automatic Documentation}: Built-in Swagger UI and OpenAPI specification generation facilitates frontend integration and testing.
    \item \textbf{Performance}: ASGI-based architecture provides high throughput suitable for inference workloads.
    \item \textbf{Simplicity}: No unnecessary features (ORM, admin) that add complexity without benefit for an ML inference API.
\end{itemize}

\subsubsection{Frontend Framework Selection}

\paragraph{Options Explored}

Three major frontend frameworks were considered:
\begin{itemize}
    \item \textbf{React}: Component-based library by Meta with extensive ecosystem.
    \item \textbf{Vue}: Progressive framework with gentle learning curve.
    \item \textbf{Angular}: Full-featured framework by Google with strong typing.
\end{itemize}

\paragraph{Comparison}

\begin{table}[H]
\centering
\caption{Frontend Framework Comparison}
\label{tab:frontend_framework}
\small
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{2.8cm}|p{3.5cm}|p{3.5cm}|p{3.2cm}|}
\hline
\textbf{Criteria} & \textbf{React} & \textbf{Vue} & \textbf{Angular} \\
\hline
TypeScript Support & Excellent & Good & Native \\
\hline
Component Model & Functional + Hooks & Options/Composition API & Class-based \\
\hline
Ecosystem Size & Largest & Large & Large \\
\hline
Learning Curve & Moderate & Low & Steep \\
\hline
Canvas Libraries & Extensive (Fabric.js, Konva) & Good & Limited \\
\hline
Build Tooling & Vite, Next.js, CRA & Vite, Nuxt & Angular CLI \\
\hline
\end{tabular}
\end{table}

\paragraph{Decision}

React with TypeScript was selected for the following reasons:
\begin{itemize}
    \item \textbf{Ecosystem}: The largest ecosystem provides libraries for every need, including canvas manipulation (critical for mask drawing in inpainting).
    \item \textbf{TypeScript Integration}: Mature TypeScript support enables type-safe development with excellent IDE support.
    \item \textbf{Hooks}: Functional components with hooks provide clean state management without class complexity.
    \item \textbf{Community}: Extensive documentation, tutorials, and community support accelerate development.
\end{itemize}

\paragraph{Supporting Tools}
\begin{itemize}
    \item \textbf{Vite}: Fast build tool with hot module replacement, significantly faster than Create React App.
    \item \textbf{Tailwind CSS}: Utility-first CSS framework enabling rapid UI development without writing custom CSS.
    \item \textbf{Canvas API}: Native HTML5 canvas for mask drawing, integrated via React refs.
\end{itemize}

\subsubsection{Deployment Strategy}

\paragraph{Options Explored}

Three deployment environments were evaluated:
\begin{itemize}
    \item \textbf{Google Colab}: Free cloud Jupyter environment with T4 GPU access.
    \item \textbf{NTU HPC Cluster}: University-provided high-performance computing resources.
    \item \textbf{Cloud VM}: Commercial cloud instances (AWS, GCP, Azure) with GPU support.
\end{itemize}

\paragraph{Comparison}

\begin{table}[H]
\centering
\caption{Deployment Environment Comparison}
\label{tab:deployment}
\small
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{2.8cm}|p{3.2cm}|p{3.2cm}|p{3.5cm}|}
\hline
\textbf{Criteria} & \textbf{Google Colab} & \textbf{NTU HPC} & \textbf{Cloud VM} \\
\hline
Cost & Free (T4) & Free (for students) & \$1--3/hour (GPU) \\
\hline
GPU Availability & T4 (15GB) & V100/A100 & Various \\
\hline
Session Limits & 12 hours, idle timeout & Queue-based & Persistent \\
\hline
Public Access & ngrok tunnelling & VPN required & Direct \\
\hline
Setup Complexity & Low (notebook) & Moderate (job scripts) & High (infrastructure) \\
\hline
\end{tabular}
\end{table}

\paragraph{Decision}

A hybrid deployment strategy was adopted:
\begin{itemize}
    \item \textbf{Development/Demo}: Google Colab with ngrok tunnelling for rapid prototyping and demonstration. The T4 GPU (15 GB VRAM) supports all models with quantisation.
    \item \textbf{Extended Testing}: NTU HPC for longer-running experiments requiring V100/A100 GPUs without session limits.
    \item \textbf{Frontend Hosting}: Static frontend deployed separately (e.g., Vercel, Netlify) to avoid session timeout issues.
\end{itemize}

\paragraph{Quantisation Strategy}

To enable deployment on memory-constrained environments like Colab's T4 GPU, DiffusionDesk implements dynamic quantisation:
\begin{itemize}
    \item \textbf{8-bit (LLM.int8)}: Reduces VRAM by $\sim$50\% with minimal quality impact. Suitable for SDXL on 8--12 GB GPUs.
    \item \textbf{4-bit (NF4)}: Reduces VRAM by $\sim$75\% with some quality trade-off. Enables FLUX.1 Fill on T4 GPU.
    \item \textbf{CPU Offloading}: Sequential layer offloading to CPU RAM when GPU VRAM is exhausted, trading speed for memory.
\end{itemize}

Users can select quantisation level per model based on their hardware constraints and quality requirements.

\newpage

% ============================================================
% 4. SOFTWARE REQUIREMENTS
% ============================================================
\section{Software Requirements}

This section presents the software requirements for DiffusionDesk, a web-based image editing application powered by diffusion models. The requirements were gathered through a combination of literature review of existing image editing tools, analysis of diffusion model capabilities, and consideration of the target deployment environment (Google Colab with NVIDIA T4 GPU). The requirements are organised into use cases, functional requirements, and non-functional requirements.

\subsection{Use Case Diagram}

Figure~\ref{fig:use_case_diagram} presents the use case diagram for DiffusionDesk, illustrating the interactions between the User actor and the system's core functionalities. The system boundary encompasses ten use cases spanning the three main feature areas: Inpainting, Style Transfer, and Restoration.

% TODO: Create use case diagram in draw.io or Lucidchart and export to images/use_case_diagram.png
% Diagram Layout Description:
% - Primary Actor (stick figure, left): User
% - Secondary Actor (stick figure, right): System
% - System Boundary (rectangle): DiffusionDesk
%
% Use Cases inside boundary:
%   UC-1: Upload Image (top centre)
%   UC-2: Perform Inpainting (left cluster)
%   UC-3: Draw Inpainting Mask (left cluster)
%   UC-4: Apply Style Transfer (centre cluster)
%   UC-5: Select Style Preset (centre cluster)
%   UC-6: Restore Image (right cluster)
%   UC-7: Select Model (bottom left)
%   UC-8: Configure Generation Parameters (bottom centre)
%   UC-9: Download Result Image (bottom right)
%   UC-10: Load Diffusion Model (right side, near System actor)
%
% Relationships:
%   Direct actor associations (User only connects to primary use cases):
%   User --> UC-2 (Perform Inpainting)
%   User --> UC-4 (Apply Style Transfer)
%   User --> UC-6 (Restore Image)
%   System --> UC-10 (Load Diffusion Model)
%
%   <<include>> relationships (required steps):
%   UC-2 <<include>> UC-1 (must upload image before inpainting)
%   UC-2 <<include>> UC-3 (must draw mask for inpainting)
%   UC-4 <<include>> UC-1 (must upload image before style transfer)
%   UC-6 <<include>> UC-1 (must upload image before restoration)
%   UC-2 <<include>> UC-10 (system loads model)
%   UC-4 <<include>> UC-10 (system loads model)
%   UC-6 <<include>> UC-10 (system loads model)
%
%   <<extend>> relationships (optional steps):
%   UC-2 <<extend>> UC-7 (optionally select model)
%   UC-4 <<extend>> UC-5 (optionally select preset)
%   UC-2 <<extend>> UC-8 (optionally configure parameters)
%   UC-4 <<extend>> UC-8 (optionally configure parameters)
%   UC-6 <<extend>> UC-8 (optionally configure parameters)
%   UC-2 <<extend>> UC-9 (optionally download result)
%   UC-4 <<extend>> UC-9 (optionally download result)
%   UC-6 <<extend>> UC-9 (optionally download result)

\begin{figure}[H]
    \centering
    % Uncomment the line below once the diagram image is created:
    % \includegraphics[width=0.85\textwidth]{use_case_diagram.png}
    \fbox{\parbox{0.85\textwidth}{\centering\vspace{4cm}\textit{Use case diagram to be inserted.\\See comments in source for diagram specification.}\vspace{4cm}}}
    \caption{Use Case Diagram for DiffusionDesk}
    \label{fig:use_case_diagram}
\end{figure}

The User actor interacts directly with three primary use cases: UC-2 (Perform Inpainting), UC-4 (Apply Style Transfer), and UC-6 (Restore Image). The remaining use cases are reached indirectly through include and extend relationships. UC-10 (Load Diffusion Model) is associated with the System actor, as it is triggered automatically when a model is needed for inference. Key relationships include:

\begin{itemize}
    \item \textbf{Include relationships}: UC-2, UC-4, and UC-6 all include UC-1 (Upload Image), as an image must be uploaded before any editing operation. UC-2 also includes UC-3 (Draw Inpainting Mask), as a mask is required for inpainting. All editing operations include UC-10 (Load Diffusion Model).
    \item \textbf{Extend relationships}: UC-7 (Select Model) and UC-8 (Configure Generation Parameters) extend the editing use cases, as users may optionally change the model or adjust parameters. UC-5 (Select Style Preset) extends UC-4 (Apply Style Transfer). UC-9 (Download Result Image) extends all three editing use cases, as the user may optionally download the generated result.
\end{itemize}

\subsubsection{Use Case Descriptions}

The following tables provide detailed descriptions for each use case identified in the use case diagram. Each table follows a standardised format documenting the use case metadata, flow of events, and related information.

\vspace{0.5em}

% ---- UC-1: Upload Image ----
\noindent\textbf{UC-1: Upload Image}

\begin{longtable}{|>{\bfseries\raggedright}p{4.2cm}|p{10.3cm}|}
\hline
Use Case ID & UC-1 \\ \hline
Use Case Name & Upload Image \\ \hline
Created By & Lee Yu Quan \\ \hline
Date Created & 8 February 2026 \\ \hline
Actor & User \\ \hline
Description & The user uploads an image from their local device to the application for subsequent editing operations (inpainting, style transfer, or restoration). \\ \hline
Preconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The user has navigated to one of the editing tabs (Inpainting, Style Transfer, or Restoration).
    \item The user has an image file in a supported format (JPEG, PNG, WebP).
\end{enumerate}\end{minipage} \\ \hline
Postconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The uploaded image is displayed as a preview in the editing canvas.
    \item The image is ready for subsequent editing operations.
\end{enumerate}\end{minipage} \\ \hline
Priority & High \\ \hline
Frequency of Use & Every editing session \\ \hline
Flow of Events & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The user clicks the upload area or drag-and-drop zone.
    \item The system opens a file selection dialog.
    \item The user selects an image file.
    \item The system validates the file format and size.
    \item The system displays the image preview in the canvas.
\end{enumerate}\end{minipage} \\ \hline
Alternative Flows & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[3a.] The user drags and drops an image file onto the upload area instead of using the file dialog.
\end{enumerate}\end{minipage} \\ \hline
Exceptions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[4a.] The file format is not supported: the system displays an error message listing supported formats.
    \item[4b.] The file exceeds the maximum size limit: the system displays an error with the size constraint.
\end{enumerate}\end{minipage} \\ \hline
Includes & -- \\ \hline
Special Requirements & Image files must not exceed 10 MB. Supported formats: JPEG, PNG, WebP. \\ \hline
Assumptions & The user has a compatible web browser with JavaScript enabled. \\ \hline
Notes and Issues & Large images may be resized on the backend to fit within model input constraints. \\ \hline
\end{longtable}

\vspace{0.5em}

% ---- UC-2: Perform Inpainting ----
\noindent\textbf{UC-2: Perform Inpainting}

\begin{longtable}{|>{\bfseries\raggedright}p{4.2cm}|p{10.3cm}|}
\hline
Use Case ID & UC-2 \\ \hline
Use Case Name & Perform Inpainting \\ \hline
Created By & Lee Yu Quan \\ \hline
Date Created & 8 February 2026 \\ \hline
Actor & User \\ \hline
Description & The user fills in or replaces a masked region of an uploaded image using a diffusion-based inpainting model. The user provides a text prompt describing the desired content for the masked area. \\ \hline
Preconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item An image has been uploaded (UC-1).
    \item A mask has been drawn over the region to inpaint (UC-3).
    \item The backend server is running and a GPU is available.
\end{enumerate}\end{minipage} \\ \hline
Postconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The inpainted image is displayed alongside the original for comparison.
    \item The result image is available for download (UC-9).
\end{enumerate}\end{minipage} \\ \hline
Priority & High \\ \hline
Frequency of Use & Frequent \\ \hline
Flow of Events & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The user navigates to the Inpainting tab.
    \item The user uploads an image (UC-1).
    \item The user draws a mask over the region to edit (UC-3).
    \item The user enters a text prompt describing the desired content.
    \item The user optionally selects a model (UC-7) and configures parameters (UC-8).
    \item The user clicks the ``Generate'' button.
    \item The system sends the image, mask, prompt, and parameters to the backend.
    \item The system loads the selected model if not already cached (UC-10).
    \item The system performs inpainting inference and returns the result.
    \item The system displays the inpainted image.
\end{enumerate}\end{minipage} \\ \hline
Alternative Flows & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[4a.] The user enters a negative prompt to specify undesired content.
    \item[10a.] The user is unsatisfied and regenerates with different parameters.
\end{enumerate}\end{minipage} \\ \hline
Exceptions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[8a.] The model fails to load due to insufficient GPU VRAM: the system suggests a lower quantisation level or smaller model.
    \item[9a.] Inference fails or times out: the system displays an error message and allows the user to retry.
\end{enumerate}\end{minipage} \\ \hline
Includes & UC-1 (Upload Image), UC-3 (Draw Inpainting Mask), UC-10 (Load Diffusion Model) \\ \hline
Special Requirements & Requires an active GPU backend. Inference time varies by model (10--120 seconds). \\ \hline
Assumptions & The backend has sufficient VRAM for the selected model and quantisation level. \\ \hline
Notes and Issues & FLUX.1 Fill models require NF4 quantisation to run on T4 GPUs. \\ \hline
\end{longtable}

\vspace{0.5em}

% ---- UC-3: Draw Inpainting Mask ----
\noindent\textbf{UC-3: Draw Inpainting Mask}

\begin{longtable}{|>{\bfseries\raggedright}p{4.2cm}|p{10.3cm}|}
\hline
Use Case ID & UC-3 \\ \hline
Use Case Name & Draw Inpainting Mask \\ \hline
Created By & Lee Yu Quan \\ \hline
Date Created & 8 February 2026 \\ \hline
Actor & User \\ \hline
Description & The user draws a binary mask on the uploaded image to indicate the region that should be inpainted. White pixels in the mask denote the area to be filled. \\ \hline
Preconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item An image has been uploaded and displayed in the Inpainting tab.
\end{enumerate}\end{minipage} \\ \hline
Postconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item A mask overlay is visible on the image canvas.
    \item The mask data is ready to be sent with the inpainting request.
\end{enumerate}\end{minipage} \\ \hline
Priority & High \\ \hline
Frequency of Use & Every inpainting operation \\ \hline
Flow of Events & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The user selects the brush tool on the canvas.
    \item The user adjusts the brush size using the slider control.
    \item The user draws over the region to be inpainted by clicking and dragging on the canvas.
    \item The system renders the mask overlay in a semi-transparent colour.
    \item The user reviews the mask coverage.
\end{enumerate}\end{minipage} \\ \hline
Alternative Flows & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[3a.] The user uses the eraser tool to remove parts of the mask.
    \item[3b.] The user clicks ``Clear Mask'' to reset the entire mask.
\end{enumerate}\end{minipage} \\ \hline
Exceptions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[3a.] No image is loaded: the canvas tools are disabled.
\end{enumerate}\end{minipage} \\ \hline
Includes & -- \\ \hline
Special Requirements & The canvas must support touch input for tablet/touchscreen devices. \\ \hline
Assumptions & The user can use a mouse or touchscreen to draw on the canvas. \\ \hline
Notes and Issues & The mask is internally represented as a black-and-white image matching the uploaded image dimensions. \\ \hline
\end{longtable}

\vspace{0.5em}

% ---- UC-4: Apply Style Transfer ----
\noindent\textbf{UC-4: Apply Style Transfer}

\begin{longtable}{|>{\bfseries\raggedright}p{4.2cm}|p{10.3cm}|}
\hline
Use Case ID & UC-4 \\ \hline
Use Case Name & Apply Style Transfer \\ \hline
Created By & Lee Yu Quan \\ \hline
Date Created & 8 February 2026 \\ \hline
Actor & User \\ \hline
Description & The user applies an artistic style (e.g., anime, oil painting, watercolour, pixel art) to an uploaded image using a diffusion-based image-to-image pipeline. \\ \hline
Preconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item An image has been uploaded (UC-1).
    \item The backend server is running and a GPU is available.
\end{enumerate}\end{minipage} \\ \hline
Postconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The stylised image is displayed alongside the original.
    \item The result image is available for download (UC-9).
\end{enumerate}\end{minipage} \\ \hline
Priority & High \\ \hline
Frequency of Use & Frequent \\ \hline
Flow of Events & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The user navigates to the Style Transfer tab.
    \item The user uploads an image (UC-1).
    \item The user selects a style preset (UC-5) or enters a custom style prompt.
    \item The user optionally adjusts the strength parameter to control style intensity.
    \item The user optionally configures additional parameters (UC-8).
    \item The user clicks the ``Generate'' button.
    \item The system sends the image, prompt, and parameters to the backend.
    \item The system loads the model if not already cached (UC-10).
    \item The system performs image-to-image inference and returns the result.
    \item The system displays the stylised image.
\end{enumerate}\end{minipage} \\ \hline
Alternative Flows & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[3a.] The user enters a custom prompt instead of selecting a preset.
    \item[10a.] The user adjusts the strength and regenerates to fine-tune the result.
\end{enumerate}\end{minipage} \\ \hline
Exceptions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[8a.] Model fails to load: the system suggests a lower quantisation level.
    \item[9a.] Inference fails: the system displays an error message.
\end{enumerate}\end{minipage} \\ \hline
Includes & UC-1 (Upload Image), UC-10 (Load Diffusion Model) \\ \hline
Special Requirements & Requires an active GPU backend. \\ \hline
Assumptions & The backend has sufficient VRAM for the SDXL img2img model. \\ \hline
Notes and Issues & Higher strength values produce more stylised results but may lose original content details. A strength of 0.5--0.7 is recommended for most styles. \\ \hline
\end{longtable}

\vspace{0.5em}

% ---- UC-5: Select Style Preset ----
\noindent\textbf{UC-5: Select Style Preset}

\begin{longtable}{|>{\bfseries\raggedright}p{4.2cm}|p{10.3cm}|}
\hline
Use Case ID & UC-5 \\ \hline
Use Case Name & Select Style Preset \\ \hline
Created By & Lee Yu Quan \\ \hline
Date Created & 8 February 2026 \\ \hline
Actor & User \\ \hline
Description & The user selects a predefined artistic style from a list of presets. Each preset maps to a curated prompt optimised for that style. \\ \hline
Preconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The user is on the Style Transfer tab.
\end{enumerate}\end{minipage} \\ \hline
Postconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The selected style preset is applied to the prompt field.
    \item The style is ready to be used in the next generation.
\end{enumerate}\end{minipage} \\ \hline
Priority & Medium \\ \hline
Frequency of Use & Frequent \\ \hline
Flow of Events & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The user views the list of available style presets (e.g., Anime, Oil Painting, Watercolour, Pixel Art, Cinematic, Pencil Sketch).
    \item The user clicks on a style preset.
    \item The system populates the prompt field with the preset's curated prompt.
\end{enumerate}\end{minipage} \\ \hline
Alternative Flows & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[3a.] The user modifies the auto-populated prompt to customise the style further.
\end{enumerate}\end{minipage} \\ \hline
Exceptions & -- \\ \hline
Includes & -- \\ \hline
Special Requirements & -- \\ \hline
Assumptions & The list of presets is fetched from the backend API. \\ \hline
Notes and Issues & Presets can be extended by adding new entries to the backend configuration. \\ \hline
\end{longtable}

\vspace{0.5em}

% ---- UC-6: Restore Image ----
\noindent\textbf{UC-6: Restore Image}

\begin{longtable}{|>{\bfseries\raggedright}p{4.2cm}|p{10.3cm}|}
\hline
Use Case ID & UC-6 \\ \hline
Use Case Name & Restore Image \\ \hline
Created By & Lee Yu Quan \\ \hline
Date Created & 8 February 2026 \\ \hline
Actor & User \\ \hline
Description & The user restores or enhances a degraded image using one of the available restoration models: CodeFormer and GFPGAN for face restoration, or Real-ESRGAN for general image upscaling and enhancement. \\ \hline
Preconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item An image has been uploaded (UC-1).
    \item The backend server is running and a GPU is available.
\end{enumerate}\end{minipage} \\ \hline
Postconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The restored image is displayed alongside the original.
    \item The result image is available for download (UC-9).
\end{enumerate}\end{minipage} \\ \hline
Priority & High \\ \hline
Frequency of Use & Frequent \\ \hline
Flow of Events & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The user navigates to the Restoration tab.
    \item The user uploads an image (UC-1).
    \item The user selects a restoration model (e.g., CodeFormer, GFPGAN, or Real-ESRGAN).
    \item The user optionally adjusts model-specific parameters (e.g., fidelity weight for CodeFormer, upscale factor for Real-ESRGAN).
    \item The user clicks the ``Restore'' button.
    \item The system sends the image and parameters to the backend.
    \item The system loads the selected model (UC-10).
    \item The system performs restoration inference and returns the result.
    \item The system displays the restored image.
\end{enumerate}\end{minipage} \\ \hline
Alternative Flows & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[3a.] The user tries a different restoration model on the same image.
\end{enumerate}\end{minipage} \\ \hline
Exceptions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[3a.] Face restoration models (CodeFormer, GFPGAN) produce suboptimal results on images without detectable faces: the system returns the image with minimal changes.
    \item[8a.] Inference fails: the system displays an error message.
\end{enumerate}\end{minipage} \\ \hline
Includes & UC-1 (Upload Image), UC-10 (Load Diffusion Model) \\ \hline
Special Requirements & Requires an active GPU backend. Face restoration models work best on images containing human faces. \\ \hline
Assumptions & The uploaded image contains content suitable for the selected restoration model. \\ \hline
Notes and Issues & Real-ESRGAN can upscale images by 2$\times$ or 4$\times$, which increases output resolution and file size. \\ \hline
\end{longtable}

\vspace{0.5em}

% ---- UC-7: Select Model ----
\noindent\textbf{UC-7: Select Model}

\begin{longtable}{|>{\bfseries\raggedright}p{4.2cm}|p{10.3cm}|}
\hline
Use Case ID & UC-7 \\ \hline
Use Case Name & Select Model \\ \hline
Created By & Lee Yu Quan \\ \hline
Date Created & 8 February 2026 \\ \hline
Actor & User \\ \hline
Description & The user selects a specific AI model from the list of available models for the current editing operation. Different models offer varying trade-offs between quality, speed, and VRAM requirements. \\ \hline
Preconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The user is on an editing tab (Inpainting, Style Transfer, or Restoration).
\end{enumerate}\end{minipage} \\ \hline
Postconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The selected model is stored and will be used for the next generation request.
\end{enumerate}\end{minipage} \\ \hline
Priority & Medium \\ \hline
Frequency of Use & Occasional \\ \hline
Flow of Events & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The user views the model selection dropdown.
    \item The system displays available models with their names.
    \item The user selects a model from the dropdown.
    \item The system updates the selected model for subsequent requests.
\end{enumerate}\end{minipage} \\ \hline
Alternative Flows & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[1a.] The user keeps the default model selection.
\end{enumerate}\end{minipage} \\ \hline
Exceptions & -- \\ \hline
Includes & -- \\ \hline
Special Requirements & The model list should be fetched dynamically from the backend API. \\ \hline
Assumptions & The backend returns a list of models appropriate for the current task. \\ \hline
Notes and Issues & Available inpainting models include SD Inpainting, SDXL Inpainting, Kandinsky, and FLUX.1 Fill. Style transfer uses SDXL img2img. Restoration models include CodeFormer, GFPGAN, and Real-ESRGAN. \\ \hline
\end{longtable}

\vspace{0.5em}

% ---- UC-8: Configure Generation Parameters ----
\noindent\textbf{UC-8: Configure Generation Parameters}

\begin{longtable}{|>{\bfseries\raggedright}p{4.2cm}|p{10.3cm}|}
\hline
Use Case ID & UC-8 \\ \hline
Use Case Name & Configure Generation Parameters \\ \hline
Created By & Lee Yu Quan \\ \hline
Date Created & 8 February 2026 \\ \hline
Actor & User \\ \hline
Description & The user adjusts generation parameters that control the behaviour and quality of the AI inference, such as guidance scale, number of inference steps, strength, and random seed. \\ \hline
Preconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The user is on an editing tab with parameter controls visible.
\end{enumerate}\end{minipage} \\ \hline
Postconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The configured parameters are stored and will be sent with the next generation request.
\end{enumerate}\end{minipage} \\ \hline
Priority & Medium \\ \hline
Frequency of Use & Occasional \\ \hline
Flow of Events & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The user expands the advanced parameters section.
    \item The user adjusts one or more parameters using sliders or input fields:
    \begin{itemize}[nosep,leftmargin=*]
        \item Guidance scale (1.0--20.0)
        \item Number of inference steps (1--100)
        \item Strength (0.0--1.0, for img2img/style transfer)
        \item Random seed (integer, or $-1$ for random)
        \item Quantisation level (none, 8-bit, 4-bit)
    \end{itemize}
    \item The system validates the parameter values against allowed ranges.
    \item The system updates the stored parameters.
\end{enumerate}\end{minipage} \\ \hline
Alternative Flows & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[1a.] The user does not expand the advanced section and uses default values.
\end{enumerate}\end{minipage} \\ \hline
Exceptions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[3a.] A parameter value is out of range: the system clamps it to the nearest valid value.
\end{enumerate}\end{minipage} \\ \hline
Includes & -- \\ \hline
Special Requirements & Parameter controls must provide sensible default values. \\ \hline
Assumptions & The user understands the effect of advanced parameters or is satisfied with defaults. \\ \hline
Notes and Issues & Increasing inference steps improves quality but increases generation time. Guidance scale controls prompt adherence. \\ \hline
\end{longtable}

\vspace{0.5em}

% ---- UC-9: Download Result Image ----
\noindent\textbf{UC-9: Download Result Image}

\begin{longtable}{|>{\bfseries\raggedright}p{4.2cm}|p{10.3cm}|}
\hline
Use Case ID & UC-9 \\ \hline
Use Case Name & Download Result Image \\ \hline
Created By & Lee Yu Quan \\ \hline
Date Created & 8 February 2026 \\ \hline
Actor & User \\ \hline
Description & The user downloads the generated or restored image to their local device. \\ \hline
Preconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item A result image has been generated by one of the editing operations.
\end{enumerate}\end{minipage} \\ \hline
Postconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The result image is saved to the user's local device.
\end{enumerate}\end{minipage} \\ \hline
Priority & Medium \\ \hline
Frequency of Use & After each successful generation \\ \hline
Flow of Events & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The user clicks the ``Download'' button below the result image.
    \item The system triggers a file download in the browser.
    \item The image is saved to the user's default download directory.
\end{enumerate}\end{minipage} \\ \hline
Alternative Flows & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[1a.] The user right-clicks the image and selects ``Save Image As'' from the browser context menu.
\end{enumerate}\end{minipage} \\ \hline
Exceptions & -- \\ \hline
Includes & -- \\ \hline
Special Requirements & The downloaded image should be in PNG format to preserve quality. \\ \hline
Assumptions & The user's browser supports programmatic file downloads. \\ \hline
Notes and Issues & The filename includes a timestamp and operation type for easy identification. \\ \hline
\end{longtable}

\vspace{0.5em}

% ---- UC-10: Load Diffusion Model ----
\noindent\textbf{UC-10: Load Diffusion Model}

\begin{longtable}{|>{\bfseries\raggedright}p{4.2cm}|p{10.3cm}|}
\hline
Use Case ID & UC-10 \\ \hline
Use Case Name & Load Diffusion Model \\ \hline
Created By & Lee Yu Quan \\ \hline
Date Created & 8 February 2026 \\ \hline
Actor & System \\ \hline
Description & The system loads the requested diffusion model into GPU memory, applying quantisation if specified. Previously loaded models are cached to avoid redundant loading. \\ \hline
Preconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item An editing operation has been requested (UC-2, UC-4, or UC-6).
    \item The requested model is not already cached in GPU memory.
\end{enumerate}\end{minipage} \\ \hline
Postconditions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The model is loaded into GPU memory and ready for inference.
    \item The model is cached for subsequent requests.
\end{enumerate}\end{minipage} \\ \hline
Priority & High \\ \hline
Frequency of Use & On first use of each model per session \\ \hline
Flow of Events & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item The system receives a generation request specifying a model.
    \item The system checks if the model is already cached in memory.
    \item If not cached, the system downloads the model weights from Hugging Face Hub (or loads from local cache).
    \item The system applies the requested quantisation level (none, 8-bit, or 4-bit).
    \item The system loads the model onto the GPU.
    \item The system caches the loaded model for future requests.
    \item The system returns control to the calling use case for inference.
\end{enumerate}\end{minipage} \\ \hline
Alternative Flows & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[2a.] The model is already cached: skip to step 7.
    \item[4a.] If the model requires CPU offloading, the system enables sequential offloading.
\end{enumerate}\end{minipage} \\ \hline
Exceptions & \begin{minipage}[t]{\linewidth}\begin{enumerate}[nosep,leftmargin=*,topsep=0pt,partopsep=0pt,after=\vspace{2pt}]
    \item[3a.] Model weights are unavailable (network error or missing from Hub): the system returns an error to the user.
    \item[5a.] Insufficient GPU VRAM: the system clears cached models and retries, or returns an error suggesting a lower quantisation level.
\end{enumerate}\end{minipage} \\ \hline
Includes & -- \\ \hline
Special Requirements & Model loading can take 30--120 seconds depending on model size and network speed. A progress indicator should be shown to the user. \\ \hline
Assumptions & The backend has internet access to download model weights on first use. \\ \hline
Notes and Issues & Switching between large models (e.g., SDXL to FLUX.1) may require clearing the previous model from VRAM first. \\ \hline
\end{longtable}

\subsection{Functional and Non-Functional Requirements}

The functional and non-functional requirements for DiffusionDesk were derived from the use case analysis above, supplemented by best practices in web application development and the specific constraints of deploying diffusion models on consumer-grade GPUs. Functional requirements define the system's behaviour, while non-functional requirements specify quality attributes and constraints.

\subsubsection{Functional Requirements}

The functional requirements are organised by use case, with each requirement identified by a unique code in the format \textbf{FR-XX}.

\vspace{0.5em}
\noindent\underline{\textbf{Upload Image}}

\begin{enumerate}[label=\textbf{FR-\arabic*}.,start=1]
    \item The system shall accept image uploads in JPEG, PNG, and WebP formats.
    \item The system shall reject image files exceeding 10 MB and display an appropriate error message.
    \item The system shall display a preview of the uploaded image in the editing canvas.
    \item The system shall support drag-and-drop image upload in addition to the file selection dialog.
    \item The system shall resize images exceeding the model's maximum input resolution while preserving the aspect ratio.
\end{enumerate}

\vspace{0.5em}
\noindent\underline{\textbf{Perform Inpainting}}

\begin{enumerate}[label=\textbf{FR-\arabic*}.,start=6]
    \item The system shall provide a selection of inpainting models (SD Inpainting, SDXL Inpainting, Kandinsky Inpainting, FLUX.1 Fill).
    \item The system shall accept a text prompt describing the desired content for the masked region.
    \item The system shall accept an optional negative prompt to specify undesired content.
    \item The system shall send the image, mask, prompt, negative prompt, and generation parameters to the backend API.
    \item The system shall display the inpainted result image alongside the original for visual comparison.
\end{enumerate}

\vspace{0.5em}
\noindent\underline{\textbf{Draw Inpainting Mask}}

\begin{enumerate}[label=\textbf{FR-\arabic*}.,start=11]
    \item The system shall provide a canvas-based brush tool for drawing masks over the uploaded image.
    \item The system shall allow the user to adjust the brush size via a slider control.
    \item The system shall render the mask as a semi-transparent overlay on the image.
    \item The system shall provide a ``Clear Mask'' button to reset the entire mask.
    \item The system shall generate a binary mask image (black background, white mask) matching the uploaded image dimensions.
\end{enumerate}

\vspace{0.5em}
\noindent\underline{\textbf{Apply Style Transfer}}

\begin{enumerate}[label=\textbf{FR-\arabic*}.,start=16]
    \item The system shall support style transfer via diffusion-based image-to-image generation.
    \item The system shall provide predefined style presets (e.g., Anime, Oil Painting, Watercolour, Pixel Art, Cinematic, Pencil Sketch).
    \item The system shall allow the user to enter a custom style prompt.
    \item The system shall provide a strength parameter (0.0--1.0) to control the intensity of the style application.
    \item The system shall display the stylised result image alongside the original for comparison.
\end{enumerate}

\vspace{0.5em}
\noindent\underline{\textbf{Restore Image}}

\begin{enumerate}[label=\textbf{FR-\arabic*}.,start=21]
    \item The system shall support face restoration using CodeFormer and GFPGAN models.
    \item The system shall support general image upscaling and enhancement using Real-ESRGAN.
    \item The system shall provide model-specific parameters (e.g., fidelity weight for CodeFormer, upscale factor for Real-ESRGAN).
    \item The system shall display the restored result image alongside the original for comparison.
\end{enumerate}

\vspace{0.5em}
\noindent\underline{\textbf{Select Model}}

\begin{enumerate}[label=\textbf{FR-\arabic*}.,start=25]
    \item The system shall fetch the list of available models from the backend API dynamically.
    \item The system shall display the available models in a dropdown selection menu.
    \item The system shall set a sensible default model for each editing tab.
\end{enumerate}

\vspace{0.5em}
\noindent\underline{\textbf{Configure Generation Parameters}}

\begin{enumerate}[label=\textbf{FR-\arabic*}.,start=28]
    \item The system shall provide controls for adjusting guidance scale (1.0--20.0).
    \item The system shall provide controls for adjusting the number of inference steps (1--100).
    \item The system shall provide controls for adjusting strength (0.0--1.0) for image-to-image operations.
    \item The system shall provide a seed input field for reproducible generation ($-1$ for random).
    \item The system shall provide quantisation level selection (none, 8-bit, 4-bit) where applicable.
    \item The system shall validate parameter values and clamp them to valid ranges.
    \item The system shall provide sensible default values for all parameters.
\end{enumerate}

\vspace{0.5em}
\noindent\underline{\textbf{Download Result Image}}

\begin{enumerate}[label=\textbf{FR-\arabic*}.,start=35]
    \item The system shall provide a download button for each generated result image.
    \item The system shall download the image in PNG format with a descriptive filename.
\end{enumerate}

\vspace{0.5em}
\noindent\underline{\textbf{Load Diffusion Model}}

\begin{enumerate}[label=\textbf{FR-\arabic*}.,start=37]
    \item The system shall load diffusion models on demand when a generation request is received.
    \item The system shall cache loaded models in GPU memory to avoid redundant loading.
    \item The system shall apply quantisation (8-bit or 4-bit) when specified by the user.
    \item The system shall clear previously cached models when GPU VRAM is insufficient for a new model.
    \item The system shall display a loading indicator while a model is being loaded.
\end{enumerate}

\subsubsection{Non-Functional Requirements}

The non-functional requirements define the quality attributes and constraints of DiffusionDesk. Each requirement is identified by a unique code in the format \textbf{NFR-XX}.

\vspace{0.5em}
\noindent\underline{\textbf{Usability}}

\begin{enumerate}[label=\textbf{NFR-\arabic*}.,start=1]
    \item The user interface shall be intuitive and require no prior experience with diffusion models to use basic features.
    \item The system shall provide clear labels, tooltips, and instructions for all controls.
    \item The system shall maintain a consistent visual design across all tabs using Tailwind CSS.
    \item The system shall provide real-time feedback during image generation, including progress indicators and status messages.
    \item The system shall organise features into clearly labelled tabs (Inpainting, Style Transfer, Restoration) for easy navigation.
    \item The system shall display error messages in a user-friendly format, avoiding raw technical error traces.
\end{enumerate}

\vspace{0.5em}
\noindent\underline{\textbf{Performance}}

\begin{enumerate}[label=\textbf{NFR-\arabic*}.,start=7]
    \item The frontend shall load within 3 seconds on a standard broadband connection.
    \item Image upload and preview rendering shall complete within 2 seconds for files under 5 MB.
    \item Inpainting and style transfer inference shall complete within 120 seconds on a T4 GPU with 4-bit quantisation.
    \item Image restoration shall complete within 30 seconds on a T4 GPU.
    \item The system shall display generation progress to the user so that long-running operations do not appear unresponsive.
\end{enumerate}

\vspace{0.5em}
\noindent\underline{\textbf{Reliability}}

\begin{enumerate}[label=\textbf{NFR-\arabic*}.,start=12]
    \item The system shall handle backend errors gracefully and display meaningful error messages to the user.
    \item The system shall recover from GPU out-of-memory errors by suggesting alternative models or quantisation levels.
    \item The frontend shall remain functional even if the backend is temporarily unavailable, allowing image upload and parameter configuration.
    \item The system shall validate all user inputs on both the frontend and backend to prevent invalid requests.
\end{enumerate}

\vspace{0.5em}
\noindent\underline{\textbf{Maintainability}}

\begin{enumerate}[label=\textbf{NFR-\arabic*}.,start=16]
    \item The system shall follow a modular architecture with clear separation between frontend, backend API, and ML inference services.
    \item The backend shall use a router-service pattern with low coupling between API endpoints and model inference logic.
    \item The frontend shall use reusable React components with well-defined props interfaces.
    \item New models shall be addable to the system by extending the backend service configuration without modifying existing code.
\end{enumerate}

\vspace{0.5em}
\noindent\underline{\textbf{Compatibility}}

\begin{enumerate}[label=\textbf{NFR-\arabic*}.,start=20]
    \item The frontend shall be compatible with the latest versions of Google Chrome, Mozilla Firefox, Microsoft Edge, and Apple Safari.
    \item The frontend shall be responsive and functional on screen widths from 1024 pixels and above.
    \item The backend shall run on NVIDIA GPUs with CUDA support (compute capability 7.0 or higher).
    \item The system shall support deployment on Google Colab with T4 GPU using ngrok for backend exposure.
\end{enumerate}

\newpage

% ============================================================
% 5. PLANNING AND DESIGN
% ============================================================
\section{Planning and Design}

This section presents the planning and design decisions made during the development of DiffusionDesk. It covers the software development methodology adopted, the system architecture, and the user interface wireframe designs that guided the implementation.

\subsection{Project Development Methodology}

The software development methodology adopted for DiffusionDesk is the \textbf{Iterative and Incremental Development} methodology. This approach was chosen because the project involves integrating multiple AI models with varying hardware requirements, where each feature (inpainting, style transfer, restoration) can be developed and tested as an independent increment. The iterative nature allows for continuous refinement based on testing feedback, which is particularly important when working with diffusion models where output quality depends heavily on parameter tuning and model selection.

% TODO: Create Agile/Iterative methodology diagram in draw.io and export to images/iterative_methodology.png
% Diagram should show the iterative cycle: Requirements  Design  Implement  Test  Evaluate  (repeat)

\begin{figure}[H]
    \centering
    % Uncomment the line below once the diagram image is created:
    % \includegraphics[width=0.7\textwidth]{iterative_methodology.png}
    \fbox{\parbox{0.7\textwidth}{\centering\vspace{3cm}\textit{Iterative development methodology diagram to be inserted.}\vspace{3cm}}}
    \caption{Iterative and Incremental Development Methodology}
    \label{fig:iterative_methodology}
\end{figure}

The project was divided into four main iterations, each building upon the previous:

\begin{enumerate}
    \item \textbf{Iteration 1 --- Project Setup and Backend Foundation}: Established the FastAPI backend structure with the router-service architecture pattern. Set up the project repository, defined API schemas using Pydantic, and implemented the health check endpoint. The frontend scaffolding was created using React, TypeScript, and Vite with Tailwind CSS for styling.

    \item \textbf{Iteration 2 --- Inpainting Feature}: Implemented the inpainting service with support for multiple diffusion models (SD Inpainting, SDXL Inpainting, Kandinsky, FLUX.1 Fill). Developed the frontend \texttt{InpaintingTab} component with the interactive \texttt{MaskCanvas} for drawing masks. Integrated quantisation support (8-bit and 4-bit) using bitsandbytes to enable deployment on memory-constrained GPUs.

    \item \textbf{Iteration 3 --- Style Transfer and Restoration Features}: Added the style transfer service using SDXL img2img pipelines with predefined style presets. Implemented the restoration service integrating CodeFormer, GFPGAN, and Real-ESRGAN models. Developed the corresponding frontend tabs (\texttt{StyleTransferTab} and \texttt{RestorationTab}).

    \item \textbf{Iteration 4 --- Integration Testing and Deployment}: Configured Google Colab deployment with ngrok for backend exposure. Tested all three features end-to-end on T4 GPU hardware. Refined parameter defaults and error handling based on testing results.
\end{enumerate}

At the end of each iteration, the working software was reviewed and evaluated against the project requirements. Feedback from testing on actual GPU hardware informed the subsequent iteration, particularly regarding VRAM constraints and quantisation strategies. This iterative approach proved well-suited for the project, as requirements around model support and parameter ranges evolved as new models were tested and evaluated.

\subsection{System Architecture}

DiffusionDesk adopts a \textbf{client-server architecture} with a clear separation between the frontend application and the backend API server. The frontend is a single-page application (SPA) built with React that communicates with the backend via RESTful HTTP requests. The backend is a FastAPI server that handles API routing, request validation, and delegates ML inference to dedicated service modules. Figure~\ref{fig:system_architecture} illustrates the overall system architecture.

% TODO: Create system architecture diagram in draw.io and export to images/system_architecture.png
% Diagram Layout:
%
%   +------------------+         HTTP/JSON         +---------------------------+
%   |   Frontend       | -----------------------> |   Backend (FastAPI)        |
%   |   (React SPA)    | <----------------------- |                           |
%   |                  |    Base64 Images +        |   +---------------------+ |
%   | +-------------+  |    JSON Params            |   | Routers             | |
%   | | InpaintingTab| |                           |   |  /api/inpainting/   | |
%   | | StyleTransfer| |                           |   |  /api/style/        | |
%   | | RestorationTab||                           |   |  /api/restoration/  | |
%   | +-------------+  |                           |   |  /api/health        | |
%   | | MaskCanvas   |  |                           |   +---------------------+ |
%   | | ImageUpload  |  |                           |            |              |
%   | +-------------+  |                           |            v              |
%   | | API Client   |  |                           |   +---------------------+ |
%   | | (imageApi.ts)|  |                           |   | Schemas (Pydantic)  | |
%   | +-------------+  |                           |   | InpaintingRequest   | |
%   +------------------+                           |   | StyleTransferRequest| |
%         |                                        |   | RestorationRequest  | |
%         | Hosted on                              |   | ImageResponse       | |
%         | Vercel/Netlify                         |   +---------------------+ |
%         | or localhost:5173                       |            |              |
%                                                  |            v              |
%                                                  |   +---------------------+ |
%                                                  |   | Services            | |
%                                                  |   |  DiffusionService   | |
%                                                  |   |  RestorationService | |
%                                                  |   +---------------------+ |
%                                                  |            |              |
%                                                  |            v              |
%                                                  |   +---------------------+ |
%                                                  |   | GPU (CUDA)          | |
%                                                  |   | PyTorch + Diffusers | |
%                                                  |   | Model Cache         | |
%                                                  |   +---------------------+ |
%                                                  +---------------------------+
%                                                        |
%                                                        | Hosted on
%                                                        | Colab + ngrok
%                                                        | or localhost:8000

\begin{figure}[H]
    \centering
    % Uncomment the line below once the diagram image is created:
    % \includegraphics[width=0.9\textwidth]{system_architecture.png}
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{5cm}\textit{System architecture diagram to be inserted.\\See comments in source for diagram specification.}\vspace{5cm}}}
    \caption{System Architecture of DiffusionDesk}
    \label{fig:system_architecture}
\end{figure}

The architecture consists of two main components: the \textbf{Frontend Application} and the \textbf{Backend API Server}, connected via HTTP/JSON communication. This separation enables independent deployment --- the frontend can be hosted on static hosting platforms such as Vercel or Netlify, while the backend runs on a GPU-equipped machine such as Google Colab.

\subsubsection{Frontend Architecture}

The frontend is a React 19 single-page application built with TypeScript and bundled using Vite. Tailwind CSS provides utility-first styling with a dark theme (slate-900 background with indigo accents). The application is organised into the following layers:

\begin{itemize}
    \item \textbf{App Component} (\texttt{App.tsx}): The root component manages tab navigation through an \texttt{activeTab} state variable. Four tabs are available: Home, Inpainting, Style Transfer, and Restoration. Each tab renders its corresponding component.

    \item \textbf{Tab Components} (\texttt{components/}): Each editing feature is encapsulated in its own tab component (\texttt{InpaintingTab.tsx}, \texttt{StyleTransferTab.tsx}, \texttt{RestorationTab.tsx}). Each tab manages its own local state for image data, generation parameters, and UI state (loading, errors, results). This design ensures that tabs are independent and do not share mutable state.

    \item \textbf{Shared Components}: Reusable components include \texttt{ImageUpload} (drag-and-drop file upload with preview) and \texttt{MaskCanvas} (HTML5 Canvas-based interactive drawing tool for creating inpainting masks). These are used across multiple tabs to maintain consistency.

    \item \textbf{API Client} (\texttt{api/}): The \texttt{imageApi.ts} module provides typed API functions (\texttt{inpaintImage()}, \texttt{styleTransfer()}, \texttt{restoreImage()}) that handle base64 encoding, request construction, and response parsing. The \texttt{config.ts} module reads the API base URL from the \texttt{VITE\_API\_URL} environment variable, enabling seamless switching between local and cloud-deployed backends.
\end{itemize}

\subsubsection{Backend Architecture}

The backend follows a \textbf{three-layer router-service-schema architecture} built on FastAPI. This pattern separates concerns cleanly and mirrors the structure used in production web services:

\begin{itemize}
    \item \textbf{Router Layer} (\texttt{routers/}): Three router modules (\texttt{inpainting.py}, \texttt{style\_transfer.py}, \texttt{restoration.py}) define the API endpoints. Each router receives HTTP requests, extracts validated parameters from Pydantic schemas, delegates processing to the corresponding service, and returns a standardised \texttt{ImageResponse}. Routers are registered in \texttt{main.py} with URL prefixes (\texttt{/api/inpainting/}, \texttt{/api/style/}, \texttt{/api/restoration/}).

    \item \textbf{Schema Layer} (\texttt{schemas/image.py}): Pydantic models define the API contracts. \texttt{InpaintingRequest}, \texttt{StyleTransferRequest}, and \texttt{RestorationRequest} validate incoming data with type checking and range constraints (e.g., \texttt{guidance\_scale} between 1.0 and 20.0, \texttt{num\_inference\_steps} between 10 and 100). The shared \texttt{ImageResponse} model provides a consistent response format with fields for \texttt{success}, \texttt{image} (base64), \texttt{error}, \texttt{model\_used}, and \texttt{processing\_time}.

    \item \textbf{Service Layer} (\texttt{services/}): The \texttt{DiffusionService} class manages diffusion model pipelines for inpainting and style transfer, while the \texttt{RestorationService} handles CodeFormer, GFPGAN, and Real-ESRGAN models. Services are instantiated as \textbf{singletons} using factory functions (\texttt{get\_diffusion\_service()}, \texttt{get\_restoration\_service()}), ensuring that loaded models are cached in GPU memory across multiple requests. This avoids redundant model loading and reduces response latency for subsequent requests.
\end{itemize}

\subsubsection{Model Management Strategy}

A key architectural decision is the \textbf{lazy loading with caching} strategy for ML models. Models are not loaded at server startup; instead, they are loaded on demand when the first request for a given model is received. Once loaded, the model pipeline is cached in a dictionary keyed by model identifier. This approach offers several benefits:

\begin{itemize}
    \item \textbf{Memory efficiency}: Only models that are actively used consume GPU VRAM.
    \item \textbf{Fast subsequent requests}: Cached models skip the loading phase (which can take 30--120 seconds) on repeat use.
    \item \textbf{Flexible model switching}: Users can select different models per request without preloading all options.
    \item \textbf{Quantisation on demand}: Models can be loaded with different precision levels (FP16, 8-bit, 4-bit NF4) based on the user's hardware constraints.
\end{itemize}

When GPU VRAM is insufficient for a new model, the system clears previously cached models before attempting to load the requested model.

\subsubsection{Communication Protocol}

The frontend and backend communicate via \textbf{RESTful JSON APIs}. Images are transmitted as base64-encoded strings within JSON payloads, which simplifies the API design by avoiding multipart form data. The data flow for a typical editing operation proceeds as follows:

\begin{enumerate}
    \item The user uploads an image file in the browser. The frontend converts it to a base64 data URL using the \texttt{FileReader} API.
    \item The user configures parameters (prompt, model, generation settings) through the UI controls.
    \item The frontend constructs a JSON payload containing the base64 image, parameters, and (for inpainting) the mask, and sends it as a POST request to the corresponding API endpoint.
    \item The backend validates the request using Pydantic schemas, decodes the base64 images to PIL Image objects, and invokes the appropriate service method.
    \item The service loads the model if needed, performs GPU inference, and encodes the result image back to base64.
    \item The backend returns an \texttt{ImageResponse} JSON object containing the result image, model used, and processing time.
    \item The frontend converts the base64 response to a data URL and displays the result alongside the original image for comparison.
\end{enumerate}

\subsubsection{Deployment Architecture}

DiffusionDesk is designed for flexible deployment across different environments:

\begin{itemize}
    \item \textbf{Local development}: The frontend runs on \texttt{localhost:5173} (Vite dev server) and the backend on \texttt{localhost:8000} (Uvicorn). CORS middleware configured with \texttt{allow\_origins=["*"]} permits cross-origin requests between the two ports.

    \item \textbf{Google Colab deployment}: The backend runs inside a Colab notebook with GPU acceleration. Ngrok exposes the FastAPI server via a public HTTPS URL. The frontend is configured to point to the ngrok URL through the \texttt{VITE\_API\_URL} environment variable and can be hosted on a static platform or run locally.

    \item \textbf{Production deployment}: The frontend is built to static files using \texttt{npm run build} and deployed to Vercel or Netlify. The backend is deployed on a GPU-equipped server (e.g., NTU HPC cluster with V100/A100 GPUs).
\end{itemize}

\subsection{User Interface Wireframe}

This section presents the wireframe designs for the key pages of the DiffusionDesk web application. These wireframes provide a visual representation of the interface's layout and functionality, serving as a blueprint for the frontend implementation. The wireframes were designed to prioritise usability, with clear separation of controls, input areas, and result displays.

\vspace{0.5em}
\noindent\underline{\textbf{Home Page}}

% TODO: Create wireframe in draw.io and export to images/wireframe_home.png
% Layout:
%   +----------------------------------------------------------+
%   | DIFFUSIONDESK   [Home] [Inpainting] [Style] [Restoration]|
%   +----------------------------------------------------------+
%   |                                                          |
%   |              Welcome to DiffusionDesk                    |
%   |     AI-Powered Image Editing with Diffusion Models       |
%   |                                                          |
%   |  +----------------+  +----------------+  +-----------+   |
%   |  | Inpainting     |  | Style Transfer |  | Restoration|  |
%   |  | Remove objects |  | Apply artistic |  | Enhance   |  |
%   |  | and fill       |  | styles to your |  | faces and |  |
%   |  | regions        |  | images         |  | upscale   |  |
%   |  +----------------+  +----------------+  +-----------+   |
%   |                                                          |
%   +----------------------------------------------------------+

\begin{figure}[H]
    \centering
    % Uncomment the line below once the wireframe image is created:
    % \includegraphics[width=0.85\textwidth]{wireframe_home.png}
    \fbox{\parbox{0.85\textwidth}{\centering\vspace{3cm}\textit{Home page wireframe to be inserted.}\vspace{3cm}}}
    \caption{Home Page (Wireframe)}
    \label{fig:wireframe_home}
\end{figure}

The Home page serves as the landing page for DiffusionDesk. It provides an overview of the application's three core features --- Inpainting, Style Transfer, and Restoration --- presented as feature cards with brief descriptions. The top navigation bar displays the application name and tab buttons for navigating to each feature. This page is designed to orient new users and provide quick access to any feature.

\vspace{0.5em}
\noindent\underline{\textbf{Inpainting Page}}

% TODO: Create wireframe in draw.io and export to images/wireframe_inpainting.png
% Layout:
%   +----------------------------------------------------------+
%   | DIFFUSIONDESK   [Home] [Inpainting] [Style] [Restoration]|
%   +----------------------------------------------------------+
%   |                                                          |
%   |  +---------------------------+  Model: [SD Inpainting v] |
%   |  |                           |                           |
%   |  |   [Image Canvas with      |  Prompt:                  |
%   |  |    Mask Overlay]           |  [Enter prompt...      ]  |
%   |  |                           |                           |
%   |  |   Brush: [====o=====]     |  Negative Prompt:         |
%   |  |   [Clear Mask]            |  [blurry, low quality  ]  |
%   |  +---------------------------+                           |
%   |                                  Advanced Settings      |
%   |  [Upload Image]                   Guidance: [===o===]    |
%   |                                   Steps:    [===o===]    |
%   |                                   Strength: [===o===]    |
%   |                                   Seed:     [_______]    |
%   |                                   Quant:    [None   v]   |
%   |                                                          |
%   |                    [Generate]                            |
%   |                                                          |
%   |  +-------------+    +-------------+                      |
%   |  | Original    |    | Result      |                      |
%   |  |             |    |             |                       |
%   |  +-------------+    +-------------+                      |
%   |                     [Download]                           |
%   +----------------------------------------------------------+

\begin{figure}[H]
    \centering
    % Uncomment the line below once the wireframe image is created:
    % \includegraphics[width=0.85\textwidth]{wireframe_inpainting.png}
    \fbox{\parbox{0.85\textwidth}{\centering\vspace{5cm}\textit{Inpainting page wireframe to be inserted.}\vspace{5cm}}}
    \caption{Inpainting Page (Wireframe)}
    \label{fig:wireframe_inpainting}
\end{figure}

The Inpainting page is the most complex interface in DiffusionDesk. The left side features the image canvas with a mask overlay, where users draw over the region to be inpainted using a configurable brush tool. A brush size slider and ``Clear Mask'' button are positioned below the canvas. The right side contains the control panel: model selection dropdown, text prompt input, negative prompt input, and a collapsible ``Advanced Settings'' section with sliders for guidance scale, inference steps, strength, seed input, and quantisation level selection. The ``Generate'' button triggers the inpainting operation, and the result is displayed below in a side-by-side comparison with the original image. A ``Download'' button allows saving the result.

The ``Generate'' button is disabled until three conditions are met: an image is uploaded, a mask is drawn, and a prompt is entered. Inline hints inform the user of any missing requirements.

\vspace{0.5em}
\noindent\underline{\textbf{Style Transfer Page}}

% TODO: Create wireframe in draw.io and export to images/wireframe_style.png
% Layout:
%   +----------------------------------------------------------+
%   | DIFFUSIONDESK   [Home] [Inpainting] [Style] [Restoration]|
%   +----------------------------------------------------------+
%   |                                                          |
%   |  [Upload Image]      Model: [SDXL img2img          v]   |
%   |  +----------------+                                      |
%   |  | Image Preview  |  Style Presets:                      |
%   |  |                |  [Anime] [Oil Painting] [Watercolor] |
%   |  +----------------+  [Pixel Art] [Cinematic] [Sketch]    |
%   |                      [Cyberpunk] [Pop Art]               |
%   |                                                          |
%   |                       Use custom prompt                 |
%   |                      [Custom prompt...               ]   |
%   |                                                          |
%   |                      Strength: [=======o===] 0.7         |
%   |                                                          |
%   |                       Advanced Settings                 |
%   |                                                          |
%   |                    [Generate]                            |
%   |                                                          |
%   |  +-------------+    +-------------+                      |
%   |  | Original    |    | Stylised    |                      |
%   |  |             |    |             |                       |
%   |  +-------------+    +-------------+                      |
%   |                     [Download]                           |
%   +----------------------------------------------------------+

\begin{figure}[H]
    \centering
    % Uncomment the line below once the wireframe image is created:
    % \includegraphics[width=0.85\textwidth]{wireframe_style.png}
    \fbox{\parbox{0.85\textwidth}{\centering\vspace{5cm}\textit{Style Transfer page wireframe to be inserted.}\vspace{5cm}}}
    \caption{Style Transfer Page (Wireframe)}
    \label{fig:wireframe_style}
\end{figure}

The Style Transfer page provides a streamlined interface for applying artistic styles to images. The left side displays the uploaded image preview with an upload control. The right side features a grid of style preset buttons (Anime, Oil Painting, Watercolour, Pixel Art, Cinematic, Pencil Sketch, Cyberpunk, Pop Art), each with a visual icon and label. Users can alternatively enable a custom prompt checkbox to enter a free-text style description. A prominent strength slider (0.0--1.0, default 0.7) controls the intensity of the style application --- lower values preserve more of the original content, while higher values produce more stylised results. The model selection dropdown and collapsible advanced settings (guidance scale, inference steps, seed) are also available. Results are displayed in the same side-by-side comparison format as the Inpainting page.

\vspace{0.5em}
\noindent\underline{\textbf{Restoration Page}}

% TODO: Create wireframe in draw.io and export to images/wireframe_restoration.png
% Layout:
%   +----------------------------------------------------------+
%   | DIFFUSIONDESK   [Home] [Inpainting] [Style] [Restoration]|
%   +----------------------------------------------------------+
%   |                                                          |
%   |  [Upload Image]                                          |
%   |  +----------------+   Restoration Options:               |
%   |  | Image Preview  |                                      |
%   |  |                |    Face Enhancement                 |
%   |  +----------------+     Model: [CodeFormer v]            |
%   |                         Fidelity: [====o====] 0.5        |
%   |                                                          |
%   |                       Upscaling:                         |
%   |                         Scale: [None] [2x] [4x]         |
%   |                                                          |
%   |                        Scratch Removal                  |
%   |                        Colorisation                     |
%   |                                                          |
%   |                    [Restore]                             |
%   |                                                          |
%   |  +-------------+    +-------------+                      |
%   |  | Original    |    | Restored    |                      |
%   |  |             |    |             |                       |
%   |  +-------------+    +-------------+                      |
%   |                     [Download]                           |
%   +----------------------------------------------------------+

\begin{figure}[H]
    \centering
    % Uncomment the line below once the wireframe image is created:
    % \includegraphics[width=0.85\textwidth]{wireframe_restoration.png}
    \fbox{\parbox{0.85\textwidth}{\centering\vspace{5cm}\textit{Restoration page wireframe to be inserted.}\vspace{5cm}}}
    \caption{Restoration Page (Wireframe)}
    \label{fig:wireframe_restoration}
\end{figure}

The Restoration page offers a checkbox-based interface for selecting restoration operations. The primary options include: Face Enhancement with a model selector (CodeFormer or GFPGAN) and a fidelity slider (for CodeFormer, controlling the balance between quality enhancement and original fidelity); Upscaling with radio buttons for scale factor (None, 2$\times$, 4$\times$) using Real-ESRGAN; and optional Scratch Removal and Colourisation toggles. The ``Restore'' button is disabled unless at least one restoration option is enabled. This page does not require a text prompt, making it simpler than the Inpainting and Style Transfer pages. Results are displayed in the same side-by-side comparison format with a download option.

\newpage

% ============================================================
% 6. IMPLEMENTATION
% ============================================================
\section{Implementation}

\subsection{Backend Development}

\subsubsection{Project Structure}
% TODO: Describe backend project structure

\subsubsection{API Design}
% TODO: Describe FastAPI endpoints and design decisions

\subsubsection{Inpainting Service}
% TODO: Describe inpainting service implementation

\subsubsection{Style Transfer Service}
% TODO: Describe style transfer service implementation

\subsubsection{Restoration Service}
% TODO: Describe restoration service implementation (CodeFormer, GFPGAN, Real-ESRGAN)

\subsubsection{Model Management and VRAM Optimization}
% TODO: Describe model loading, quantization (4-bit, 8-bit), CPU offloading

\subsection{Frontend Development}

\subsubsection{Project Structure}
% TODO: Describe frontend project structure

\subsubsection{User Interface Design}
% TODO: Describe React components, tab navigation, Tailwind CSS styling

\subsubsection{Canvas and Mask Drawing}
% TODO: Describe inpainting mask drawing implementation

\subsubsection{API Integration}
% TODO: Describe frontend-backend communication

\newpage

% ============================================================
% 7. PROJECT DIFFICULTIES AND LEARNING OUTCOMES
% ============================================================
\section{Project Difficulties and Learning Outcomes}

\subsection{Project Difficulties}
% TODO: Describe challenges encountered

\subsection{Learning Outcomes}
% TODO: Describe what was learned

\newpage

% ============================================================
% 8. FUTURE IMPLEMENTATION
% ============================================================
\section{Future Implementation}
% TODO: Describe future work and enhancements

\newpage

% ============================================================
% 9. REFERENCES
% ============================================================
\bibliographystyle{apalike}
\bibliography{references}

\end{document}
