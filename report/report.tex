\documentclass[12pt,a4paper]{report}

% ============================================================
% Packages
% ============================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}                    % Times New Roman font
\usepackage[margin=1in]{geometry}     % 1 inch margins
\usepackage{graphicx}                 % Images
\usepackage{float}                    % Figure placement
\usepackage{caption}                  % Captions
\usepackage{subcaption}               % Subfigures
\usepackage{hyperref}                 % Hyperlinks
\usepackage{setspace}                 % Line spacing
\usepackage{titlesec}                 % Section formatting
\usepackage{tocloft}                  % TOC formatting
\usepackage{fancyhdr}                 % Headers and footers
\usepackage{tabularx}                 % Tables
\usepackage{booktabs}                 % Better table rules
\usepackage{enumitem}                 % List formatting
\usepackage{amsmath}                  % Math
\usepackage{listings}                 % Code listings
\usepackage{xcolor}                   % Colors
\usepackage{url}                      % URL formatting
\usepackage{pdfpages}                 % Include PDF pages
\usepackage{longtable}                % Multi-page tables
\usepackage{array}                    % Extended column definitions
\usepackage[authoryear,round]{natbib}  % APA-style author-year citations

% ============================================================
% Configuration
% ============================================================
\graphicspath{{images/}}
\onehalfspacing

% Hyperlink styling
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=black,
    urlcolor=blue,
    citecolor=black,
}

% Code listing style
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    tabsize=2,
    captionpos=b,
}

% Header/footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}

% Section numbering: use 1, 2, 3 instead of 0.1, 0.2
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\arabic{subsubsection}}

% Section formatting
\titleformat{\section}
  {\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}

% Remove chapter numbering (we use sections directly)
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{3}

% ============================================================
% Document
% ============================================================
\begin{document}

% ============================================================
% TITLE PAGE
% ============================================================
\begin{titlepage}
    \begin{center}
        \vspace*{1cm}

        % NTU Logo
        \includegraphics[width=0.35\textwidth]{ntu_logo.png}

        \vspace{1cm}

        {\Large \textbf{NANYANG TECHNOLOGICAL UNIVERSITY}}

        \vspace{0.5cm}

        {\large SCHOOL OF COMPUTER SCIENCE AND ENGINEERING}

        \vspace{2cm}

        {\LARGE \textbf{Diffusion Models for Intelligent\\Image Editing and Inpainting}}

        \vspace{2cm}

        {\large
        Lee Yu Quan\\
        }

        \vspace{2cm}

        {\large
        \textbf{Supervisor:} Prof Zhang Hanwang\\
        }

        \vspace{1cm}

        {\large
        School of Computer Science and Engineering\\
        }

        \vspace{1.5cm}

        {\large
        A Final Year Project report\\
        presented to Nanyang Technological University\\
        in partial fulfilment of the requirements for the\\
        degree of Bachelor of Engineering\\
        }

        \vspace{1cm}

        {\large 2025/2026}

    \end{center}
\end{titlepage}

% ============================================================
% ACKNOWLEDGEMENTS
% ============================================================
\pagenumbering{roman}
\setcounter{page}{2}

\section*{Acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

Over the course of two semesters working on this final year project, I would like to express my appreciation to everyone who has encouraged me and offered their guidance, helping make this project possible.

I would like to extend my sincere gratitude to my supervisor, Prof Zhang Hanwang, for granting me the freedom to steer the direction of this project. His trust and openness allowed me to explore a wide variety of diffusion models and techniques in the field of image generation, which greatly enriched both the project and my learning experience.

Lastly, I would like to thank my examiner, Prof (placeholder), for taking the time to review and evaluate this final year project.

\vspace{1cm}
\noindent Lee Yu Quan\\
March 2026

\newpage

% ============================================================
% TABLE OF CONTENTS
% ============================================================
\tableofcontents
\newpage

% ============================================================
% TABLE OF FIGURES
% ============================================================
\listoffigures
\addcontentsline{toc}{section}{Table of Figures}
\newpage

% ============================================================
% LIST OF TABLES (optional)
% ============================================================
\listoftables
\addcontentsline{toc}{section}{List of Tables}
\newpage

% ============================================================
% MAIN BODY
% ============================================================
\pagenumbering{arabic}
\setcounter{page}{1}

% ============================================================
% 1. INTRODUCTION
% ============================================================
\section{Introduction}

\subsection{Abstract}
% TODO: Write abstract

\subsection{Background and Motivation}

Large language models are the most discussed aspect of generative AI today, but image generation is not far behind. According to Grand View Research, the global AI image generator market was valued at USD 349.6 million in 2023 and is projected to grow at a compound annual growth rate (CAGR) of 17.7\% to reach USD 1.08 billion by 2030 \citep{grandviewresearch2023}. Yet, there remains a lack of comprehensive software that caters to this growing demand in an accessible manner.

Traditional methods in image inpainting, such as patch-based and exemplar-based approaches, have notable limitations in generating semantically meaningful content, particularly in high-resolution or complex scenarios \citep{ma2023uncertainty}. These methods often struggle with boundary artefacts when dealing with large masked regions due to insufficient constraints, resulting in visible seams and structurally inconsistent outputs \citep{ma2023uncertainty}. Such limitations create significant accessibility barriers, as current solutions frequently require extensive technical expertise and expensive software licences, effectively restricting advanced image editing capabilities to professional users.

The introduction of deep learning techniques, particularly diffusion models \citep{ho2020ddpm, song2021ddim, rombach2022ldm}, has led to significant improvements in image generation quality and semantic comprehension, enabling capabilities that were previously difficult or impossible to automate. A detailed review of these developments is presented in Section~\ref{sec:literature_review}.

Despite these breakthroughs, critical gaps persist between state-of-the-art image editing models and users' practical needs. Existing solutions face four key limitations: (1) fragmented ecosystems requiring users to switch between different applications for different editing tasks, (2) high complexity barriers that make advanced editing tools inaccessible to non-expert users, (3) reliance on command-line tools, Python programming, and GPU-equipped hardware, and (4) a lack of unified platforms that integrate multiple diffusion model capabilities within a single interface.

This project, DiffusionDesk, addresses these gaps by deploying a web-based platform that integrates diffusion models for inpainting, style transfer, and image restoration within a single, user-friendly interface. Developed as a Final Year Project at Nanyang Technological University under the supervision of Prof Zhang Hanwang, the application provides three core image editing capabilities:

\begin{enumerate}
    \item \textbf{Inpainting} -- removing or replacing objects within selected regions of an image using models such as Stable Diffusion, Stable Diffusion XL, Kandinsky, and FLUX.1 Fill.
    \item \textbf{Style Transfer} -- applying artistic styles such as anime, oil painting, and watercolour to images using diffusion-based image-to-image translation.
    \item \textbf{Restoration} -- enhancing image quality through face restoration (CodeFormer, GFPGAN) and image upscaling (Real-ESRGAN).
\end{enumerate}

By exposing these models through a FastAPI backend and a React-based browser frontend, DiffusionDesk aims to make diffusion-based image editing accessible to users without requiring direct interaction with the underlying models or command-line tools.

\subsection{Project Objective}

The objective of this project is to design and deploy a web-based image editing platform that leverages open-source diffusion models to provide intelligent inpainting, style transfer, and image restoration capabilities. The specific objectives are as follows:

\begin{enumerate}
    \item To develop a responsive web application that integrates multiple diffusion models for image editing within a unified interface.
    \item To implement an inpainting feature that enables users to selectively remove or replace objects in images using state-of-the-art diffusion models, including Stable Diffusion, Stable Diffusion XL, Kandinsky, and FLUX.1 Fill.
    \item To implement a style transfer feature that allows users to apply artistic styles to images through diffusion-based image-to-image translation.
    \item To implement an image restoration feature that enhances image quality through face restoration and upscaling using CodeFormer, GFPGAN, and Real-ESRGAN.
    \item To design an intuitive user interface that enables non-expert users to perform advanced image editing tasks without requiring technical expertise in machine learning or programming.
    \item To evaluate the system's performance through processing speed benchmarks, output quality assessments, and usability considerations.
\end{enumerate}

\subsection{Limitations}

This project is subject to the following limitations:

\begin{enumerate}
    \item \textbf{Open-source models only} -- The application exclusively utilises open-source diffusion models available through the Hugging Face ecosystem. Proprietary or commercially licensed models are not included, which may limit the range of available capabilities compared to commercial solutions.
    \item \textbf{GPU resource constraints} -- Diffusion model inference is computationally intensive and requires GPU acceleration. The available GPU memory (VRAM) constrains the size and complexity of models that can be loaded simultaneously. Quantisation techniques (4-bit, 8-bit) are employed to mitigate this, but may result in slight quality degradation.
    \item \textbf{Supported image formats} -- The application supports JPEG, JPG, and PNG image formats only. Other formats such as TIFF, BMP, WebP, or RAW are not supported.
    \item \textbf{No mobile application} -- The platform is designed as a web application accessible through desktop and mobile browsers. A dedicated native mobile application is not within the project scope.
    \item \textbf{No video processing} -- The system processes individual images only. Video frame processing, video inpainting, or video style transfer are not supported.
    \item \textbf{No 3D image manipulation} -- The application is limited to 2D image editing. 3D reconstruction, 3D-aware editing, or depth-based manipulation are not included.
    \item \textbf{Inference only} -- The project focuses on model inference using pre-trained models. Model training, fine-tuning, or custom model development are outside the project scope.
\end{enumerate}

\subsection{Project Scope}

The scope of this project encompasses the following:

\subsubsection{In Scope}
\begin{itemize}
    \item Development of a web-based frontend using React, TypeScript, and Tailwind CSS that provides an intuitive user interface for all three editing features.
    \item Development of a backend API using FastAPI and PyTorch that serves diffusion model inference for inpainting, style transfer, and image restoration.
    \item Implementation of inpainting using Stable Diffusion Inpainting, Stable Diffusion XL Inpainting, Kandinsky Inpainting, and FLUX.1 Fill models.
    \item Implementation of style transfer using SDXL image-to-image generation with artistic style prompts.
    \item Implementation of image restoration using CodeFormer, GFPGAN (face restoration), and Real-ESRGAN (image upscaling).
    \item Support for JPEG, JPG, and PNG image formats.
    \item VRAM optimisation through model quantisation (4-bit, 8-bit) and CPU offloading to accommodate varying GPU configurations.
    \item Deployment and testing on cloud GPU environments such as Google Colab.
\end{itemize}

\subsubsection{Out of Scope}
\begin{itemize}
    \item Native mobile application development.
    \item Video processing, video inpainting, or video style transfer.
    \item 3D image manipulation or depth-based editing.
    \item Model training, fine-tuning, or custom model development.
    \item User authentication, user account management, or multi-user collaboration features.
    \item Image formats other than JPEG, JPG, and PNG.
\end{itemize}

Success will be measured through processing speed benchmarks, output quality assessments, and user experience evaluation across the three core features.

\newpage

% ============================================================
% 2. PROJECT SCHEDULE
% ============================================================
\section{Project Schedule}

This section outlines the project timeline, work breakdown structure, and risk management plan for DiffusionDesk. The project spans two semesters of Academic Year 2025/2026, with key milestones aligned to the FYP submission deadlines.

\subsection{Project Timeline}

Table~\ref{tab:timeline} presents the key milestones and deliverables for the project.

\begin{table}[H]
\centering
\caption{Project Timeline and Milestones}
\label{tab:timeline}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{3cm}|p{3cm}|p{7.5cm}|}
\hline
\textbf{Date} & \textbf{Week} & \textbf{Milestone} \\
\hline
11 Aug 2025 & Sem 1, Wk 1 & FYP officially commences \\
\hline
1 Sep 2025 & Sem 1, Wk 4 & Submission of Project Plan to Supervisor \\
\hline
Oct 2025 & Sem 1, Wk 8--10 & Backend API and diffusion service implementation \\
\hline
Nov 2025 & Sem 1, Wk 12--13 & Frontend development and API integration \\
\hline
26 Jan 2026 & Sem 2, Wk 3 & Submission of Interim Report \\
\hline
Feb 2026 & Sem 2, Wk 5--7 & Feature refinement and testing on cloud GPU \\
\hline
23 Mar 2026 & Sem 2, Wk 10 & Submission of Final Report \\
\hline
17 Apr 2026 & Sem 2, Wk 13 & Submission of Amended Final Report \\
\hline
8--13 May 2026 & -- & Oral Presentation (20 min + 10 min Q\&A) \\
\hline
\end{tabular}
\end{table}

\subsection{Work Breakdown}

This section provides a structured breakdown of the project activities, their descriptions, estimated effort, and dependencies. The work breakdown structure facilitates progress tracking and resource allocation throughout the development lifecycle.

\small
\renewcommand{\arraystretch}{1.3}
\begin{longtable}{|p{0.6cm}|p{2.6cm}|p{5.8cm}|p{1.4cm}|p{1.8cm}|}
\caption{Work Breakdown Structure} \label{tab:wbs} \\
\hline
\textbf{ID} & \textbf{Activity} & \textbf{Description} & \textbf{Effort (Days)} & \textbf{Depends On} \\
\hline
\endfirsthead
\hline
\textbf{ID} & \textbf{Activity} & \textbf{Description} & \textbf{Effort (Days)} & \textbf{Depends On} \\
\hline
\endhead
\hline
\endfoot

1.1 & Literature Review & Review foundational papers on diffusion models (DDPM, DDIM, LDM) and existing inpainting, style transfer, and restoration techniques. & 10 & -- \\
\hline
1.2 & Technology Evaluation & Evaluate and select appropriate frameworks, libraries, and pre-trained models from the Hugging Face ecosystem. & 5 & 1.1 \\
\hline
1.3 & Requirements Specification & Define functional and non-functional requirements based on project objectives and supervisor feedback. & 4 & 1.2 \\
\hline
2.1 & Backend Architecture Design & Design the FastAPI backend structure, including API endpoints, service layers, and model management strategy. & 6 & 1.3 \\
\hline
2.2 & Inpainting Service & Implement the inpainting service supporting SD, SDXL, Kandinsky, and FLUX.1 Fill models with quantisation support. & 15 & 2.1 \\
\hline
2.3 & Style Transfer Service & Implement the style transfer service using SDXL img2img with configurable style prompts and parameters. & 10 & 2.1 \\
\hline
2.4 & Restoration Service & Implement face restoration (CodeFormer, GFPGAN) and image upscaling (Real-ESRGAN) services. & 10 & 2.1 \\
\hline
2.5 & VRAM Optimisation & Implement model quantisation (4-bit, 8-bit) and CPU offloading strategies to support varying GPU configurations. & 8 & 2.2, 2.3 \\
\hline
3.1 & Frontend UI Design & Design the user interface layout, including wireframes for the Home, Inpainting, Style Transfer, and Restoration tabs. & 5 & 1.3 \\
\hline
3.2 & Frontend Implementation & Develop the React frontend with TypeScript and Tailwind CSS, implementing all UI components and tab navigation. & 15 & 3.1 \\
\hline
3.3 & Canvas and Mask Drawing & Implement the interactive canvas component for users to draw inpainting masks on uploaded images. & 8 & 3.2 \\
\hline
3.4 & API Integration & Connect the frontend to the backend API, implementing image upload, processing requests, and result display. & 6 & 2.2--2.4, 3.2 \\
\hline
4.1 & Cloud Deployment Testing & Deploy and test the backend on Google Colab with ngrok tunnelling to validate GPU inference performance. & 5 & 3.4 \\
\hline
4.2 & Feature Testing & Conduct end-to-end testing of all features, addressing bugs and refining based on test results. & 10 & 4.1 \\
\hline
4.3 & Performance Benchmarking & Measure and document inference times, memory usage, and output quality across different models. & 5 & 4.2 \\
\hline
5.1 & Supervisor Meetings & Regular meetings with the supervisor to report progress, seek guidance, and align on project direction. & 8 & All \\
\hline
5.2 & Interim Report Writing & Prepare and submit the interim report documenting progress, challenges, and preliminary results. & 5 & 2.5, 3.4 \\
\hline
5.3 & Final Report Writing & Prepare the final report with comprehensive documentation of system design, implementation, and evaluation. & 15 & 4.3, 5.2 \\
\hline
5.4 & Oral Presentation Prep & Prepare presentation slides and rehearse for the oral examination. & 5 & 5.3 \\
\hline

\end{longtable}
\normalsize

\subsection{Risk Management}

This section identifies potential risks that may impact the project's success and outlines mitigation strategies. Each risk is assessed based on its probability of occurrence, potential impact, and overall risk level. Proactive risk management ensures that challenges are anticipated and addressed promptly.

\footnotesize
\renewcommand{\arraystretch}{1.4}
\setlength{\tabcolsep}{5pt}
\begin{longtable}{|>{\raggedright\arraybackslash}p{2.2cm}|>{\raggedright\arraybackslash}p{6.2cm}|c|c|c|}
\caption{Risk Management Plan} \label{tab:risk} \\
\hline
\textbf{Risk} & \textbf{Mitigation Strategy} & \textbf{Prob.} & \textbf{Impact} & \textbf{Level} \\
\hline
\endfirsthead
\hline
\textbf{Risk} & \textbf{Mitigation Strategy} & \textbf{Prob.} & \textbf{Impact} & \textbf{Level} \\
\hline
\endhead
\hline
\endfoot

Insufficient GPU memory for large models & Implement model quantisation (4-bit, 8-bit) using bitsandbytes and enable CPU offloading. Prioritise smaller models when VRAM is limited. & High & High & High \\
\hline
Slow model inference & Use DDIM sampling with reduced steps (20--50), enable attention slicing, and implement model caching to reduce loading overhead. & Medium & High & Medium \\
\hline
Low-quality or artefacted outputs & Fine-tune prompt engineering, adjust guidance scale and denoising strength, and provide users with parameter controls for iterative refinement. & Medium & Medium & Medium \\
\hline
Diffusers library breaking changes & Pin specific library versions in requirements.txt and test compatibility before updating dependencies. & Medium & Medium & Medium \\
\hline
Cloud GPU constraints (Colab limits) & Design for stateless operation, allowing sessions to restart without data loss. Document alternative deployment options (NTU HPC, local GPU). & Medium & Medium & Medium \\
\hline
Frontend-backend integration issues & Define clear API contracts using OpenAPI, implement comprehensive error handling, and test integration incrementally. & Medium & High & Medium \\
\hline
Scope creep & Adhere strictly to defined scope. Evaluate new requests against timeline constraints and prioritise core features. & Medium & Medium & Medium \\
\hline
Unfamiliarity with diffusion architectures & Conduct thorough literature review early. Leverage tutorials, documentation, and pre-trained models to accelerate learning. & Low & High & Low \\
\hline
Time management challenges & Maintain detailed project schedule with milestones. Allocate buffer time and communicate proactively with supervisor. & Low & High & Low \\
\hline

\end{longtable}
\normalsize
\setlength{\tabcolsep}{6pt}

\newpage

% ============================================================
% 3. LITERATURE REVIEW
% ============================================================
\section{Literature Review}
\label{sec:literature_review}

\subsection{Diffusion Models}

The foundation of modern image generation lies in diffusion models, which produce images through an iterative denoising process. This subsection reviews the key developments that underpin the models used in this project.

\subsubsection{Denoising Diffusion Probabilistic Models (DDPM)}

\citet{ho2020ddpm} proposed Denoising Diffusion Probabilistic Models (DDPMs), which generate images by treating the process as a series of denoising steps grounded in nonequilibrium thermodynamics. The forward process gradually adds Gaussian noise to an image over $T$ timesteps until the image becomes pure noise. The reverse process then learns to denoise step by step, recovering a clean image from random noise. DDPMs demonstrated image quality that surpassed the then-dominant Generative Adversarial Networks (GANs), producing diverse, high-fidelity samples without the training instability commonly associated with GANs. However, the original DDPM formulation required a large number of denoising steps (typically $T = 1000$), resulting in slow sampling speeds.

\subsubsection{Denoising Diffusion Implicit Models (DDIM)}

\citet{song2021ddim} addressed the slow sampling limitation of DDPMs by proposing Denoising Diffusion Implicit Models (DDIMs). DDIMs reformulate the reverse diffusion process as a non-Markovian process, meaning that each denoising step can depend on the original noisy input rather than solely on the immediately preceding step. This reformulation allows for deterministic sampling and, crucially, enables the use of a subsequence of only 20--100 steps while maintaining comparable image quality. The result is a 10--50$\times$ speedup over DDPMs, making diffusion-based generation significantly more practical for interactive applications.

\subsubsection{Latent Diffusion Models (LDM)}

\citet{rombach2022ldm} proposed Latent Diffusion Models (LDMs), which achieved an optimal balance between generative quality and computational efficiency. Rather than performing diffusion directly in pixel space, LDMs first encode images into a lower-dimensional latent representation using a pre-trained autoencoder, then apply the diffusion process within this compressed latent space. This approach alleviates critical computational bottlenecks, substantially reducing memory and computation requirements while preserving high image quality. LDMs form the basis of the widely adopted Stable Diffusion family of models, including the inpainting and image-to-image variants used in this project.

\subsubsection{Enabling Technologies for Conditional Generation}

The diffusion models reviewed above provide the foundational denoising mechanism, but practical text-to-image systems require additional components for conditional generation. This subsection reviews three key enabling technologies that bridge the gap between unconditional diffusion and controllable image synthesis.

\paragraph{Classifier-Free Guidance}

\citet{ho2022cfg} introduced classifier-free guidance, a technique that enables conditional diffusion models to produce outputs strongly aligned with input conditions (e.g., text prompts) without requiring a separate classifier network. The method trains a single neural network to perform both conditional and unconditional generation by randomly dropping the conditioning signal during training. At inference time, the model's output is extrapolated away from the unconditional prediction towards the conditional prediction, controlled by a guidance scale parameter $w$:
\begin{equation}
\tilde{\epsilon}_\theta(z_t, c) = \epsilon_\theta(z_t, \emptyset) + w \cdot (\epsilon_\theta(z_t, c) - \epsilon_\theta(z_t, \emptyset))
\end{equation}
where $\epsilon_\theta(z_t, c)$ is the conditional prediction, $\epsilon_\theta(z_t, \emptyset)$ is the unconditional prediction, and $w$ is the guidance scale. Higher values of $w$ produce outputs more faithful to the conditioning but may reduce diversity and introduce artefacts. This parameter is exposed in DiffusionDesk as ``guidance scale'' and is fundamental to all inpainting and style transfer operations.

\paragraph{CLIP Text-Image Alignment}

\citet{radford2021clip} developed CLIP (Contrastive Language-Image Pre-training), a model trained on 400 million image-text pairs to learn a joint embedding space for images and text. CLIP's text encoder transforms natural language prompts into dense vector representations that can guide image generation. In Stable Diffusion and similar models, the CLIP text encoder processes user prompts into conditioning embeddings that direct the denoising process. This architecture enables intuitive text-based control: users describe their desired output in natural language, and the model generates images aligned with that description. SDXL extends this by using dual text encoders (CLIP ViT-L and OpenCLIP ViT-G) for richer text understanding.

\paragraph{Diffusion Transformers (DiT)}

While Stable Diffusion and SDXL use U-Net architectures as their denoising backbone, \citet{peebles2023dit} demonstrated that Vision Transformers can serve as effective diffusion backbones. Diffusion Transformers (DiT) replace the convolutional U-Net with a transformer architecture, operating on sequences of latent patches. This approach scales more efficiently with model size and has led to state-of-the-art image generation quality. FLUX.1, developed by Black Forest Labs, adopts a DiT-based architecture with flow matching (a generalisation of diffusion that learns direct probability paths rather than score functions), achieving superior quality at the cost of increased computational requirements. The architectural shift from U-Net to DiT represents a significant evolution in diffusion model design.

\vspace{1em}
\noindent\textit{Having established the theoretical foundations---from DDPM's denoising mechanism, through DDIM's accelerated sampling, to LDM's latent space operation and the enabling technologies of classifier-free guidance, CLIP conditioning, and transformer backbones---the following sections examine how these principles are instantiated in practical models for specific image editing tasks.}

\subsection{Diffusion-Based Image Inpainting}

Image inpainting is the task of filling in missing or masked regions of an image with plausible content. Traditional approaches include patch-based methods that copy similar patches from elsewhere in the image, and exemplar-based techniques that iteratively fill regions based on surrounding texture and structure. While effective for simple cases, these methods struggle with large masked regions, complex scenes, and semantically meaningful content generation---they cannot, for example, intelligently fill a masked region with a contextually appropriate object.

Deep learning approaches, particularly diffusion models, have transformed inpainting by learning rich semantic priors from large datasets. Rather than relying on local texture matching, diffusion-based inpainting models understand scene context and can generate novel content that is both visually coherent and semantically meaningful. This subsection reviews four diffusion-based inpainting models implemented in DiffusionDesk, each building upon the theoretical foundations established in Section~3.1.

\subsubsection{Problem Definition}

Given an input image $x$ and a binary mask $m$ indicating the region to be inpainted, the goal is to generate an output image $\hat{x}$ where the masked region contains plausible content consistent with the surrounding context. In diffusion-based inpainting, the model is additionally conditioned on a text prompt $c$ that guides the generation. The inpainting process can be formulated as:
\begin{equation}
\hat{x} = f(x, m, c; \theta)
\end{equation}
where $f$ is the inpainting model with parameters $\theta$. The model must preserve the unmasked regions exactly while generating coherent content in the masked areas.

\subsubsection{Inpainting Model Architectures}

\paragraph{Stable Diffusion Inpainting}

Stable Diffusion Inpainting \citep{rombach2022ldm} extends the base Stable Diffusion v1.5 model for inpainting by modifying the input architecture. While the original model accepts a 4-channel latent input, the inpainting variant accepts 9 channels: 4 for the noisy latent, 4 for the encoded masked image, and 1 for the downsampled mask. This architectural modification allows the model to explicitly condition on both the masked image content and the mask geometry.

The model inherits the core DDPM/DDIM denoising mechanism and operates in the latent space as per LDM principles. Text conditioning is provided through the CLIP ViT-L/14 text encoder, and classifier-free guidance controls the fidelity to the text prompt. With approximately 860 million parameters and VRAM requirements of 5--7 GB in FP16 precision, Stable Diffusion Inpainting offers a good balance of quality and computational efficiency, making it suitable for users with limited GPU resources.

\paragraph{Stable Diffusion XL Inpainting}

SDXL Inpainting builds upon the base SDXL architecture, which introduced several improvements over Stable Diffusion v1.5: a larger U-Net backbone with approximately 2.6 billion parameters, native 1024$\times$1024 resolution support, and dual text encoders (CLIP ViT-L and OpenCLIP ViT-G) for enhanced prompt understanding. These improvements translate to higher-quality inpainting results with better detail preservation and more accurate text-to-image alignment.

The theoretical foundations remain consistent: SDXL Inpainting uses DDPM-based denoising, supports DDIM and other accelerated schedulers, operates in latent space, and employs classifier-free guidance. However, the increased model capacity comes at a computational cost---SDXL Inpainting requires 10--12 GB VRAM in FP16 precision. To address this, DiffusionDesk implements 8-bit and 4-bit quantisation using the bitsandbytes library, reducing VRAM requirements to approximately 6 GB and 4 GB respectively, with minimal quality degradation.

\paragraph{Kandinsky 2.2 Inpainting}

Kandinsky 2.2 employs a distinct two-stage architecture inspired by unCLIP/DALL-E 2. The first stage (``prior'') maps the text prompt to a CLIP image embedding, predicting what the image embedding should look like given the text. The second stage (``decoder'') generates the actual image conditioned on both the text and the predicted image embedding. This architecture leverages CLIP's powerful joint text-image space to guide generation.

For inpainting, Kandinsky conditions the decoder on the masked image and mask geometry in addition to the CLIP embeddings. With approximately 2 billion parameters across both stages and VRAM requirements of 6--8 GB, Kandinsky offers comparable resource usage to Stable Diffusion while providing different aesthetic characteristics. The two-stage approach can produce results with strong text alignment due to the explicit CLIP image embedding conditioning.

\paragraph{FLUX.1 Fill}

FLUX.1 Fill, developed by Black Forest Labs, represents a significant architectural departure from the U-Net-based models. Built on the Diffusion Transformer (DiT) architecture, FLUX.1 uses a transformer backbone operating on sequences of latent patches rather than a convolutional U-Net. Additionally, FLUX.1 employs flow matching rather than traditional score-based diffusion, learning direct probability paths between noise and data distributions.

With approximately 12 billion parameters and a T5-XXL text encoder (rather than CLIP), FLUX.1 Fill achieves state-of-the-art inpainting quality with exceptional detail, coherence, and prompt following. However, this quality comes at significant computational cost: full BF16 inference requires 22--24 GB VRAM. DiffusionDesk implements NF4 quantisation to reduce this to approximately 10 GB, enabling deployment on consumer GPUs like the NVIDIA T4 available in Google Colab.

\subsubsection{Model Comparison}

Table~\ref{tab:inpainting_comparison} summarises the key characteristics of the four inpainting models implemented in DiffusionDesk.

\begin{table}[H]
\centering
\caption{Comparison of Inpainting Models}
\label{tab:inpainting_comparison}
\small
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{2.2cm}|p{2.5cm}|p{2.8cm}|p{2.5cm}|p{2.8cm}|}
\hline
\textbf{Criteria} & \textbf{SD Inpainting} & \textbf{SDXL Inpainting} & \textbf{Kandinsky 2.2} & \textbf{FLUX.1 Fill} \\
\hline
Architecture & U-Net (LDM) & U-Net (LDM) & Prior + Decoder & DiT (Transformer) \\
\hline
Parameters & $\sim$860M & $\sim$2.6B & $\sim$2B & $\sim$12B \\
\hline
Text Encoder & CLIP ViT-L & CLIP + OpenCLIP & CLIP & T5-XXL \\
\hline
VRAM (FP16) & 5--7 GB & 10--12 GB & 6--8 GB & 22--24 GB \\
\hline
VRAM (4-bit) & N/A & $\sim$4 GB & N/A & $\sim$10 GB \\
\hline
Quality & Good & Very Good & Good & Excellent \\
\hline
Theory Basis & DDPM, DDIM, LDM, CFG & DDPM, DDIM, LDM, CFG & DDPM, DDIM, LDM, CLIP & Flow Matching, DiT \\
\hline
\end{tabular}
\end{table}

\subsubsection{Selection Justification}

DiffusionDesk includes all four inpainting models to provide users with flexibility across different use cases and hardware constraints:

\begin{itemize}
    \item \textbf{SD Inpainting}: Entry-level option for users with limited VRAM ($<$8 GB). Provides good quality results with fast inference.
    \item \textbf{SDXL Inpainting}: Recommended default for users with mid-range GPUs (8--12 GB). Offers improved quality and prompt understanding with quantisation options for constrained environments.
    \item \textbf{Kandinsky 2.2}: Alternative architecture providing different aesthetic characteristics. Useful when SDXL results are unsatisfactory for specific prompts.
    \item \textbf{FLUX.1 Fill}: State-of-the-art quality for users with high-end GPUs or when quantised deployment is acceptable. Best choice for complex inpainting tasks requiring maximum coherence.
\end{itemize}

\noindent\textit{The diffusion architectures examined for inpainting can also be applied to style transfer. Rather than filling masked regions, style transfer leverages the same denoising process to transform an entire image's aesthetic while preserving its structure.}

\subsection{Diffusion-Based Style Transfer}

Style transfer aims to render an image in a different artistic style while preserving its underlying content and structure. This section reviews the evolution of neural style transfer techniques and explains how diffusion models provide a powerful and flexible approach through the image-to-image (img2img) mechanism.

\subsubsection{Evolution of Neural Style Transfer}

\citet{gatys2016style} pioneered neural style transfer by demonstrating that convolutional neural networks (CNNs) encode both content and style information in their hierarchical feature representations. Their optimisation-based approach iteratively modifies an image to match the content features of a source image and the style features (captured via Gram matrices) of a reference style image. While producing impressive results, this method requires a slow optimisation process for each image pair.

Subsequent work developed feed-forward style transfer networks that train a single network per style, enabling real-time inference. However, these methods are limited to pre-defined styles and cannot generalise to arbitrary style descriptions. More recent approaches explored arbitrary style transfer using adaptive instance normalisation or attention mechanisms, but these often struggle with complex style semantics or structural preservation.

Diffusion models offer a compelling alternative: rather than explicitly separating content and style features, they leverage the denoising process with text conditioning to apply stylistic transformations described in natural language. This approach supports arbitrary styles without retraining and benefits from the semantic understanding encoded in large-scale text-image models.

\subsubsection{Image-to-Image Mechanism}

Diffusion-based style transfer operates through the image-to-image (img2img) pipeline, which differs fundamentally from text-to-image generation. Rather than starting from pure random noise, img2img begins by adding controlled noise to the input image and then denoises with a style-describing prompt. This process can be understood as follows:

\begin{enumerate}
    \item \textbf{Encoding}: The input image is encoded into latent space using the VAE encoder.
    \item \textbf{Noise Addition}: Gaussian noise is added to the latent representation according to a specified ``denoising strength'' parameter $s \in [0, 1]$. Higher values add more noise, starting the denoising from a later timestep.
    \item \textbf{Conditional Denoising}: The noised latent is denoised using the diffusion model, conditioned on a text prompt describing the desired style (e.g., ``oil painting style'', ``anime illustration'').
    \item \textbf{Decoding}: The denoised latent is decoded back to pixel space.
\end{enumerate}

The denoising strength parameter directly controls the trade-off between structure preservation and style application:
\begin{itemize}
    \item \textbf{Low strength (0.2--0.4)}: Subtle stylisation; original composition, shapes, and details are largely preserved.
    \item \textbf{Medium strength (0.5--0.7)}: Moderate stylisation; structural elements remain recognisable but undergo significant aesthetic transformation.
    \item \textbf{High strength (0.8--1.0)}: Aggressive stylisation; the output may diverge substantially from the input, using it primarily as compositional guidance.
\end{itemize}

This mechanism directly leverages the DDPM/DDIM denoising process: the noise schedule determines at which timestep the denoising begins, and classifier-free guidance ensures the output adheres to the style prompt.

\subsubsection{Approach Selection}

DiffusionDesk implements style transfer using SDXL img2img for the following reasons:

\begin{itemize}
    \item \textbf{Quality}: SDXL's larger capacity and dual text encoders produce higher-quality stylisations with better detail and prompt following compared to SD 1.5.
    \item \textbf{Resolution}: Native 1024$\times$1024 support enables high-resolution style transfer without tiling artefacts.
    \item \textbf{Flexibility}: Text-based style conditioning supports arbitrary styles (anime, oil painting, watercolour, pencil sketch, cyberpunk, etc.) without requiring style reference images.
    \item \textbf{Parameter Control}: Exposing denoising strength, guidance scale, and inference steps gives users fine-grained control over the structure-style trade-off.
    \item \textbf{Quantisation Support}: 8-bit and 4-bit quantisation via bitsandbytes enables deployment on resource-constrained GPUs.
\end{itemize}

DiffusionDesk provides preset style prompts (anime, oil painting, watercolour, pencil sketch, digital art) while allowing users to specify custom prompts for specialised styles.

\vspace{1em}
\noindent\textit{While diffusion models excel at generative tasks like inpainting and style transfer, image restoration requires specialised architectures optimised for specific degradation types. The following section examines restoration models that complement diffusion-based editing.}

\subsection{Image Restoration}

Image restoration addresses the inverse problem of recovering high-quality images from degraded observations. Common degradations include compression artefacts, noise, blur, low resolution, and facial damage (wrinkles, scratches, low quality). Unlike the generative tasks of inpainting and style transfer, restoration aims to recover or enhance existing content rather than synthesise new content.

Notably, the restoration models reviewed in this section are \textit{not} diffusion models. They employ distinct architectures---codebook-based transformers, generative adversarial networks (GANs), and enhanced super-resolution networks---each optimised for specific restoration tasks. DiffusionDesk includes these models to provide a comprehensive image editing pipeline that addresses both generative and restorative user needs.

\subsubsection{Face Restoration}

Face restoration is a specialised task that recovers high-quality facial details from degraded face images. The challenge lies in reconstructing plausible facial features (eyes, nose, mouth) while maintaining the subject's identity. DiffusionDesk implements two complementary approaches: CodeFormer and GFPGAN.

\paragraph{CodeFormer}

\citet{zhou2022codeformer} proposed CodeFormer, a transformer-based face restoration method that leverages a learned discrete codebook of high-quality facial features. The approach consists of three components:

\begin{enumerate}
    \item \textbf{Codebook Learning}: A vector-quantised autoencoder learns a discrete codebook capturing diverse high-quality facial features from a large face dataset.
    \item \textbf{Code Prediction}: Given a degraded face image, a transformer predicts the sequence of codebook indices that best represent the underlying face.
    \item \textbf{Controllable Decoding}: The decoder reconstructs the face from the predicted codes, with a fidelity parameter $w \in [0, 1]$ controlling the trade-off between restoration quality and identity preservation.
\end{enumerate}

The fidelity parameter is particularly valuable: setting $w = 0$ produces maximum restoration quality (potentially altering facial features), while $w = 1$ maximally preserves the input identity (with less aggressive restoration). Users can adjust this parameter based on whether they prioritise visual quality or identity fidelity.

CodeFormer excels at restoring severely degraded faces, producing natural-looking results with realistic textures. It handles diverse degradation types robustly and provides controllable output through the fidelity parameter.

\paragraph{GFPGAN}

\citet{wang2021gfpgan} developed GFPGAN (Generative Facial Prior GAN), which incorporates rich facial priors from a pre-trained face generation model (StyleGAN2) to assist face restoration. The key innovations include:

\begin{enumerate}
    \item \textbf{Generative Facial Prior}: Channel-split spatial feature transforms inject pre-trained StyleGAN2 features to provide high-quality facial priors.
    \item \textbf{Facial Component Dictionaries}: Pre-computed dictionaries of facial components (left eye, right eye, mouth) enable component-specific enhancement.
    \item \textbf{Identity-Preserving Loss}: An identity loss term encourages the restored face to match the input identity.
\end{enumerate}

GFPGAN tends to produce sharper results than CodeFormer in some cases but may occasionally alter facial features more aggressively. It performs particularly well on moderately degraded faces and old photographs.

\paragraph{Face Restoration Comparison}

Table~\ref{tab:face_restoration} compares CodeFormer and GFPGAN across key characteristics.

\begin{table}[H]
\centering
\caption{Comparison of Face Restoration Models}
\label{tab:face_restoration}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{3.5cm}|p{5cm}|p{5cm}|}
\hline
\textbf{Criteria} & \textbf{CodeFormer} & \textbf{GFPGAN} \\
\hline
Architecture & Transformer + Codebook & GAN + StyleGAN2 Prior \\
\hline
Controllability & Fidelity parameter (0--1) & Fixed \\
\hline
Severe Degradation & Excellent & Good \\
\hline
Identity Preservation & Controllable & Moderate \\
\hline
Output Sharpness & Natural & Sharp \\
\hline
VRAM Usage & 2--4 GB & 2--4 GB \\
\hline
\end{tabular}
\end{table}

DiffusionDesk includes both models to accommodate different user preferences and degradation scenarios. CodeFormer is recommended for severely degraded images or when identity preservation is critical, while GFPGAN is suitable for general enhancement of moderately degraded faces.

\subsubsection{Image Upscaling}

Image upscaling (super-resolution) increases image resolution while adding plausible high-frequency details. Classical interpolation methods (bicubic, bilinear) produce blurry results, while deep learning approaches can hallucinate realistic details.

\paragraph{Real-ESRGAN}

\citet{wang2021realesrgan} extended ESRGAN (Enhanced Super-Resolution GAN) to handle real-world degradations. Unlike prior methods trained on synthetic bicubic downsampling, Real-ESRGAN models a comprehensive degradation pipeline including blur, noise, compression, and their combinations. This training strategy enables robust performance on diverse real-world images.

Key characteristics of Real-ESRGAN include:

\begin{itemize}
    \item \textbf{RRDB Architecture}: Residual-in-Residual Dense Blocks provide powerful feature extraction for upscaling.
    \item \textbf{Real-World Degradation Model}: Training includes blur kernels, noise injection, JPEG compression, and resize operations in various orders.
    \item \textbf{Scale Options}: Supports 2$\times$ and 4$\times$ upscaling with pre-trained models.
    \item \textbf{Face Enhancement Integration}: A face-enhanced variant (Real-ESRGAN + GFPGAN) applies face restoration after upscaling for improved facial detail.
\end{itemize}

Real-ESRGAN produces sharp, detailed upscaled images with minimal artefacts, making it suitable for enhancing low-resolution photographs, enlarging images for printing, or improving image quality before further editing.

\subsubsection{Selection Justification}

DiffusionDesk includes CodeFormer, GFPGAN, and Real-ESRGAN to provide comprehensive restoration capabilities:

\begin{itemize}
    \item \textbf{CodeFormer}: Primary face restoration model offering controllable quality-fidelity trade-off.
    \item \textbf{GFPGAN}: Alternative face restoration for users preferring sharper outputs or as a fallback when CodeFormer results are unsatisfactory.
    \item \textbf{Real-ESRGAN}: Essential for image upscaling, complementing face restoration for full-image enhancement pipelines.
\end{itemize}

These restoration models integrate seamlessly with diffusion-based editing: users can upscale low-resolution images before inpainting, or restore faces after style transfer to recover facial details.

\subsection{Technology Stack}

This section reviews and justifies the technology choices for developing DiffusionDesk. Each subsection follows a structured approach: exploring available options, comparing them against relevant criteria, and justifying the final selection.

\subsubsection{Machine Learning Framework Selection}

\paragraph{Options Explored}

Two major deep learning frameworks were considered for model inference:
\begin{itemize}
    \item \textbf{PyTorch}: Developed by Meta AI, known for dynamic computation graphs, Pythonic API, and strong research community adoption.
    \item \textbf{TensorFlow}: Developed by Google, known for production deployment tools, static graph optimisation, and TensorFlow Serving.
\end{itemize}

\paragraph{Comparison}

\begin{table}[H]
\centering
\caption{ML Framework Comparison}
\label{tab:ml_framework}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{4cm}|p{4.5cm}|p{4.5cm}|}
\hline
\textbf{Criteria} & \textbf{PyTorch} & \textbf{TensorFlow} \\
\hline
Diffusion Model Support & Excellent (Diffusers, native) & Limited (some ports exist) \\
\hline
Pre-trained Models & Extensive (Hugging Face Hub) & Moderate \\
\hline
Dynamic Graphs & Native & TF2 eager mode (added later) \\
\hline
Debugging & Straightforward (Python) & More complex (graph mode) \\
\hline
Research Adoption & Dominant in generative AI & Strong in production ML \\
\hline
Quantisation Libraries & bitsandbytes, GPTQ, AWQ & TensorFlow Lite \\
\hline
\end{tabular}
\end{table}

\paragraph{Decision}

PyTorch was selected as the ML framework for the following reasons:
\begin{itemize}
    \item \textbf{Diffusers Library}: The Hugging Face Diffusers library, which provides unified pipeline abstractions for all diffusion models used in this project, is built on PyTorch.
    \item \textbf{Model Availability}: All target models (SD, SDXL, Kandinsky, FLUX.1, CodeFormer, GFPGAN, Real-ESRGAN) have official PyTorch implementations available on Hugging Face Hub.
    \item \textbf{Quantisation Support}: The bitsandbytes library for 8-bit and 4-bit quantisation is PyTorch-native and essential for deploying large models on resource-constrained GPUs.
    \item \textbf{Research Alignment}: PyTorch dominates generative AI research, ensuring access to the latest models and techniques.
\end{itemize}

\paragraph{Supporting Libraries}

The following PyTorch ecosystem libraries are utilised:
\begin{itemize}
    \item \textbf{Diffusers}: Unified pipeline API for diffusion models, scheduler implementations (DDIM, DPM++, Euler), and model loading utilities.
    \item \textbf{Transformers}: Text encoder loading (CLIP, T5) for conditioning diffusion models.
    \item \textbf{bitsandbytes}: 8-bit (LLM.int8) and 4-bit (NF4) quantisation for reduced VRAM usage.
    \item \textbf{Accelerate}: Device placement and mixed-precision inference utilities.
\end{itemize}

\subsubsection{Backend Framework Selection}

\paragraph{Options Explored}

Three Python web frameworks were considered for the API backend:
\begin{itemize}
    \item \textbf{FastAPI}: Modern, async-first framework with automatic OpenAPI documentation.
    \item \textbf{Flask}: Lightweight, mature framework with extensive ecosystem.
    \item \textbf{Django}: Full-featured framework with ORM, admin interface, and batteries-included philosophy.
\end{itemize}

\paragraph{Comparison}

\begin{table}[H]
\centering
\caption{Backend Framework Comparison}
\label{tab:backend_framework}
\small
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{2.8cm}|p{3.5cm}|p{3.2cm}|p{3.5cm}|}
\hline
\textbf{Criteria} & \textbf{FastAPI} & \textbf{Flask} & \textbf{Django} \\
\hline
Async Support & Native (ASGI) & Extension (async views) & Limited (ASGI adapter) \\
\hline
Type Hints & Native (Pydantic) & Manual & Manual \\
\hline
Auto Documentation & OpenAPI + Swagger UI & Extension required & Extension required \\
\hline
Performance & High (async I/O) & Moderate & Moderate \\
\hline
Learning Curve & Low & Low & Moderate \\
\hline
ML Ecosystem Fit & Excellent & Good & Moderate (ORM overhead) \\
\hline
\end{tabular}
\end{table}

\paragraph{Decision}

FastAPI was selected for the following reasons:
\begin{itemize}
    \item \textbf{Async Support}: Native async/await enables efficient handling of long-running inference requests without blocking other clients.
    \item \textbf{Pydantic Integration}: Type-validated request/response models ensure robust API contracts and clear documentation.
    \item \textbf{Automatic Documentation}: Built-in Swagger UI and OpenAPI specification generation facilitates frontend integration and testing.
    \item \textbf{Performance}: ASGI-based architecture provides high throughput suitable for inference workloads.
    \item \textbf{Simplicity}: No unnecessary features (ORM, admin) that add complexity without benefit for an ML inference API.
\end{itemize}

\subsubsection{Frontend Framework Selection}

\paragraph{Options Explored}

Three major frontend frameworks were considered:
\begin{itemize}
    \item \textbf{React}: Component-based library by Meta with extensive ecosystem.
    \item \textbf{Vue}: Progressive framework with gentle learning curve.
    \item \textbf{Angular}: Full-featured framework by Google with strong typing.
\end{itemize}

\paragraph{Comparison}

\begin{table}[H]
\centering
\caption{Frontend Framework Comparison}
\label{tab:frontend_framework}
\small
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{2.8cm}|p{3.5cm}|p{3.5cm}|p{3.2cm}|}
\hline
\textbf{Criteria} & \textbf{React} & \textbf{Vue} & \textbf{Angular} \\
\hline
TypeScript Support & Excellent & Good & Native \\
\hline
Component Model & Functional + Hooks & Options/Composition API & Class-based \\
\hline
Ecosystem Size & Largest & Large & Large \\
\hline
Learning Curve & Moderate & Low & Steep \\
\hline
Canvas Libraries & Extensive (Fabric.js, Konva) & Good & Limited \\
\hline
Build Tooling & Vite, Next.js, CRA & Vite, Nuxt & Angular CLI \\
\hline
\end{tabular}
\end{table}

\paragraph{Decision}

React with TypeScript was selected for the following reasons:
\begin{itemize}
    \item \textbf{Ecosystem}: The largest ecosystem provides libraries for every need, including canvas manipulation (critical for mask drawing in inpainting).
    \item \textbf{TypeScript Integration}: Mature TypeScript support enables type-safe development with excellent IDE support.
    \item \textbf{Hooks}: Functional components with hooks provide clean state management without class complexity.
    \item \textbf{Community}: Extensive documentation, tutorials, and community support accelerate development.
\end{itemize}

\paragraph{Supporting Tools}
\begin{itemize}
    \item \textbf{Vite}: Fast build tool with hot module replacement, significantly faster than Create React App.
    \item \textbf{Tailwind CSS}: Utility-first CSS framework enabling rapid UI development without writing custom CSS.
    \item \textbf{Canvas API}: Native HTML5 canvas for mask drawing, integrated via React refs.
\end{itemize}

\subsubsection{Deployment Strategy}

\paragraph{Options Explored}

Three deployment environments were evaluated:
\begin{itemize}
    \item \textbf{Google Colab}: Free cloud Jupyter environment with T4 GPU access.
    \item \textbf{NTU HPC Cluster}: University-provided high-performance computing resources.
    \item \textbf{Cloud VM}: Commercial cloud instances (AWS, GCP, Azure) with GPU support.
\end{itemize}

\paragraph{Comparison}

\begin{table}[H]
\centering
\caption{Deployment Environment Comparison}
\label{tab:deployment}
\small
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{2.8cm}|p{3.2cm}|p{3.2cm}|p{3.5cm}|}
\hline
\textbf{Criteria} & \textbf{Google Colab} & \textbf{NTU HPC} & \textbf{Cloud VM} \\
\hline
Cost & Free (T4) & Free (for students) & \$1--3/hour (GPU) \\
\hline
GPU Availability & T4 (15GB) & V100/A100 & Various \\
\hline
Session Limits & 12 hours, idle timeout & Queue-based & Persistent \\
\hline
Public Access & ngrok tunnelling & VPN required & Direct \\
\hline
Setup Complexity & Low (notebook) & Moderate (job scripts) & High (infrastructure) \\
\hline
\end{tabular}
\end{table}

\paragraph{Decision}

A hybrid deployment strategy was adopted:
\begin{itemize}
    \item \textbf{Development/Demo}: Google Colab with ngrok tunnelling for rapid prototyping and demonstration. The T4 GPU (15 GB VRAM) supports all models with quantisation.
    \item \textbf{Extended Testing}: NTU HPC for longer-running experiments requiring V100/A100 GPUs without session limits.
    \item \textbf{Frontend Hosting}: Static frontend deployed separately (e.g., Vercel, Netlify) to avoid session timeout issues.
\end{itemize}

\paragraph{Quantisation Strategy}

To enable deployment on memory-constrained environments like Colab's T4 GPU, DiffusionDesk implements dynamic quantisation:
\begin{itemize}
    \item \textbf{8-bit (LLM.int8)}: Reduces VRAM by $\sim$50\% with minimal quality impact. Suitable for SDXL on 8--12 GB GPUs.
    \item \textbf{4-bit (NF4)}: Reduces VRAM by $\sim$75\% with some quality trade-off. Enables FLUX.1 Fill on T4 GPU.
    \item \textbf{CPU Offloading}: Sequential layer offloading to CPU RAM when GPU VRAM is exhausted, trading speed for memory.
\end{itemize}

Users can select quantisation level per model based on their hardware constraints and quality requirements.

\newpage

% ============================================================
% 4. SOFTWARE REQUIREMENTS
% ============================================================
\section{Software Requirements}

\subsection{Use Case Diagram}
% TODO: Include use case diagram

\subsubsection{Use Case Descriptions}
% TODO: Write use case descriptions

\subsection{Functional and Non-Functional Requirements}

\subsubsection{Functional Requirements}
% TODO: List functional requirements

\subsubsection{Non-Functional Requirements}
% TODO: List non-functional requirements

\newpage

% ============================================================
% 5. PLANNING AND DESIGN
% ============================================================
\section{Planning and Design}

\subsection{Project Development Methodology}
% TODO: Describe development methodology (Agile, iterative, etc.)

\subsection{System Architecture}
% TODO: Include system architecture diagram and description

\subsection{User Interface Wireframe}
% TODO: Include UI wireframes/mockups

\newpage

% ============================================================
% 6. IMPLEMENTATION
% ============================================================
\section{Implementation}

\subsection{Backend Development}

\subsubsection{Project Structure}
% TODO: Describe backend project structure

\subsubsection{API Design}
% TODO: Describe FastAPI endpoints and design decisions

\subsubsection{Inpainting Service}
% TODO: Describe inpainting service implementation

\subsubsection{Style Transfer Service}
% TODO: Describe style transfer service implementation

\subsubsection{Restoration Service}
% TODO: Describe restoration service implementation (CodeFormer, GFPGAN, Real-ESRGAN)

\subsubsection{Model Management and VRAM Optimization}
% TODO: Describe model loading, quantization (4-bit, 8-bit), CPU offloading

\subsection{Frontend Development}

\subsubsection{Project Structure}
% TODO: Describe frontend project structure

\subsubsection{User Interface Design}
% TODO: Describe React components, tab navigation, Tailwind CSS styling

\subsubsection{Canvas and Mask Drawing}
% TODO: Describe inpainting mask drawing implementation

\subsubsection{API Integration}
% TODO: Describe frontend-backend communication

\newpage

% ============================================================
% 7. PROJECT DIFFICULTIES AND LEARNING OUTCOMES
% ============================================================
\section{Project Difficulties and Learning Outcomes}

\subsection{Project Difficulties}
% TODO: Describe challenges encountered

\subsection{Learning Outcomes}
% TODO: Describe what was learned

\newpage

% ============================================================
% 8. FUTURE IMPLEMENTATION
% ============================================================
\section{Future Implementation}
% TODO: Describe future work and enhancements

\newpage

% ============================================================
% 9. REFERENCES
% ============================================================
\bibliographystyle{apalike}
\bibliography{references}

\end{document}
